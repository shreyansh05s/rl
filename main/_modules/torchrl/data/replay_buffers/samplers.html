


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchrl.data.replay_buffers.samplers &mdash; torchrl main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','UA-117752657-2');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="../../../../../versions.html"><span style="font-size:110%">main (0.6.1+133d709) &#x25BC</span></a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-0.html">Get started with Environments, TED and transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-1.html">Get started with TorchRL’s modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-2.html">Getting started with model optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-3.html">Get started with data collection and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-4.html">Get started with logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-5.html">Get started with your own first training loop</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/coding_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/torchrl_demo.html">Introduction to TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/multiagent_ppo.html">Multi-Agent Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/torchrl_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/pretrained_models.html">Using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/dqn_with_rnn.html">Recurrent DQN: Training recurrent policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/rb_tutorial.html">Using Replay Buffers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/export.html">Exporting TorchRL modules</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/multiagent_competitive_ddpg.html">Competitive Multi-Agent Reinforcement Learning (DDPG) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/coding_ddpg.html">TorchRL objectives: Coding a DDPG loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/coding_dqn.html">TorchRL trainer: A DQN example</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../reference/index.html">API Reference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../reference/knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
      <li>torchrl.data.replay_buffers.samplers</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
    
    
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=UA-117752657-2"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchrl.data.replay_buffers.samplers</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the MIT license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">textwrap</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">copy</span><span class="p">,</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">multiprocessing.context</span> <span class="kn">import</span> <span class="n">get_spawning_popen</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">tensordict</span> <span class="kn">import</span> <span class="n">MemoryMappedTensor</span><span class="p">,</span> <span class="n">TensorDict</span>
<span class="kn">from</span> <span class="nn">tensordict.utils</span> <span class="kn">import</span> <span class="n">NestedKey</span>

<span class="kn">from</span> <span class="nn">torchrl._extension</span> <span class="kn">import</span> <span class="n">EXTENSION_WARNING</span>

<span class="kn">from</span> <span class="nn">torchrl._utils</span> <span class="kn">import</span> <span class="n">_replace_last</span><span class="p">,</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">torchrl.data.replay_buffers.storages</span> <span class="kn">import</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">StorageEnsemble</span><span class="p">,</span> <span class="n">TensorStorage</span>
<span class="kn">from</span> <span class="nn">torchrl.data.replay_buffers.utils</span> <span class="kn">import</span> <span class="n">_is_int</span><span class="p">,</span> <span class="n">unravel_index</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torchrl._torchrl</span> <span class="kn">import</span> <span class="p">(</span>
        <span class="n">MinSegmentTreeFp32</span><span class="p">,</span>
        <span class="n">MinSegmentTreeFp64</span><span class="p">,</span>
        <span class="n">SumSegmentTreeFp32</span><span class="p">,</span>
        <span class="n">SumSegmentTreeFp64</span><span class="p">,</span>
    <span class="p">)</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">EXTENSION_WARNING</span><span class="p">)</span>

<span class="n">_EMPTY_STORAGE_ERROR</span> <span class="o">=</span> <span class="s2">&quot;Cannot sample from an empty storage.&quot;</span>


<div class="viewcode-block" id="Sampler"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.Sampler.html#torchrl.data.replay_buffers.Sampler">[docs]</a><span class="k">class</span> <span class="nc">Sampler</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A generic sampler base class for composable Replay Buffers.&quot;&quot;&quot;</span>

    <span class="c1"># Some samplers - mainly those without replacement -</span>
    <span class="c1"># need to keep track of the number of remaining batches</span>
    <span class="n">_remaining_batches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>

    <span class="c1"># The RNG is set by the replay buffer</span>
    <span class="n">_rng</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">update_priority</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">index</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">priority</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Calling update_priority() on a sampler </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> that is not prioritized. Make sure this is the indented behavior.&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">mark_update</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="o">*</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">default_priority</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1.0</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="o">...</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="o">...</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">ran_out</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="c1"># by default, samplers never run out</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">()&quot;</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_rng&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">state</span></div>


<div class="viewcode-block" id="RandomSampler"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.RandomSampler.html#torchrl.data.replay_buffers.RandomSampler">[docs]</a><span class="k">class</span> <span class="nc">RandomSampler</span><span class="p">(</span><span class="n">Sampler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A uniformly random sampler for composable replay buffers.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_size (int, optional): if provided, the batch size to be used by</span>
<span class="sd">            the replay buffer when calling :meth:`~.ReplayBuffer.sample`.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">_EMPTY_STORAGE_ERROR</span><span class="p">)</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">_rand_given_ndim</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">index</span><span class="p">,</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="c1"># no op</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="c1"># no op</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span></div>


<div class="viewcode-block" id="SamplerWithoutReplacement"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.SamplerWithoutReplacement.html#torchrl.data.replay_buffers.SamplerWithoutReplacement">[docs]</a><span class="k">class</span> <span class="nc">SamplerWithoutReplacement</span><span class="p">(</span><span class="n">Sampler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A data-consuming sampler that ensures that the same sample is not present in consecutive batches.</span>

<span class="sd">    Args:</span>
<span class="sd">        drop_last (bool, optional): if ``True``, the last incomplete sample (if any) will be dropped.</span>
<span class="sd">            If ``False``, this last sample will be kept and (unlike with torch dataloaders)</span>
<span class="sd">            completed with other samples from a fresh indices permutation.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        shuffle (bool, optional): if ``False``, the items are not randomly</span>
<span class="sd">            permuted. This enables to iterate over the replay buffer in the</span>
<span class="sd">            order the data was collected. Defaults to ``True``.</span>

<span class="sd">    *Caution*: If the size of the storage changes in between two calls, the samples will be re-shuffled</span>
<span class="sd">    (as we can&#39;t generally keep track of which samples have been sampled before and which haven&#39;t).</span>

<span class="sd">    Similarly, it is expected that the storage content remains the same in between two calls,</span>
<span class="sd">    but this is not enforced.</span>

<span class="sd">    When the sampler reaches the end of the list of available indices, a new sample order</span>
<span class="sd">    will be generated and the resulting indices will be completed with this new draw, which</span>
<span class="sd">    can lead to duplicated indices, unless the :obj:`drop_last` argument is set to ``True``.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span> <span class="o">=</span> <span class="n">drop_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ran_out</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span> <span class="o">=</span> <span class="n">shuffle</span>

    <span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="n">path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">TensorDict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="n">sd</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="o">.</span><span class="n">load_memmap</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">sd</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_sample_list</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">len_storage</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">storage</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="o">.</span><span class="n">device</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="s2">&quot;device&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">:</span>
            <span class="n">_sample_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span>
                <span class="n">len_storage</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rng</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_sample_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">len_storage</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span> <span class="o">=</span> <span class="n">_sample_list</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_remaining_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">//</span> <span class="n">batch_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_remaining_batches</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">//</span> <span class="o">-</span><span class="n">batch_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_single_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">len_storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="p">[:</span><span class="n">batch_size</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="p">[</span><span class="n">batch_size</span><span class="p">:]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_remaining_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">//</span> <span class="n">batch_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_remaining_batches</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">//</span> <span class="o">-</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="c1"># check if we have enough elements for one more batch, assuming same batch size</span>
        <span class="c1"># will be used each time sample is called</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">batch_size</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ran_out</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_sample_list</span><span class="p">(</span>
                <span class="n">storage</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">len_storage</span><span class="o">=</span><span class="n">len_storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ran_out</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="n">index</span>

    <span class="k">def</span> <span class="nf">_storage_len</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>  <span class="c1"># noqa: F811</span>
        <span class="n">len_storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage_len</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">len_storage</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">_EMPTY_STORAGE_ERROR</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">len_storage</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;An empty storage was passed&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span> <span class="o">!=</span> <span class="n">len_storage</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_sample_list</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">len_storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">len_storage</span> <span class="o">&lt;</span> <span class="n">batch_size</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The batch size (</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">) is greater than the storage capacity (</span><span class="si">{</span><span class="n">len_storage</span><span class="si">}</span><span class="s2">). &quot;</span>
                <span class="s2">&quot;This makes it impossible to return a sample without repeating indices. &quot;</span>
                <span class="s2">&quot;Consider changing the sampler class or turn the &#39;drop_last&#39; argument to False.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span> <span class="o">=</span> <span class="n">len_storage</span>
        <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_single_sample</span><span class="p">(</span><span class="n">len_storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">storage</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">unravel_index</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">storage</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="c1"># we &#39;always&#39; return the indices. The &#39;drop_last&#39; just instructs the</span>
        <span class="c1"># sampler to turn to `ran_out = True` whenever the next sample</span>
        <span class="c1"># will be too short. This will be read by the replay buffer</span>
        <span class="c1"># as a signal for an early break of the __iter__().</span>
        <span class="k">return</span> <span class="n">index</span><span class="p">,</span> <span class="p">{}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">ran_out</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ran_out</span>

    <span class="nd">@ran_out</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">ran_out</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ran_out</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ran_out</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">OrderedDict</span><span class="p">(</span>
            <span class="n">len_storage</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span><span class="p">,</span>
            <span class="n">_sample_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="p">,</span>
            <span class="n">drop_last</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span><span class="p">,</span>
            <span class="n">_ran_out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_ran_out</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;len_storage&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_sample_list&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;drop_last&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ran_out</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_ran_out&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">perc</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span> <span class="o">*</span> <span class="mi">100</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">perc</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">perc</span><span class="si">:</span><span class="s2"> 4.4f</span><span class="si">}</span><span class="s2">% sampled)&quot;</span></div>


<div class="viewcode-block" id="PrioritizedSampler"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.PrioritizedSampler.html#torchrl.data.replay_buffers.PrioritizedSampler">[docs]</a><span class="k">class</span> <span class="nc">PrioritizedSampler</span><span class="p">(</span><span class="n">Sampler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Prioritized sampler for replay buffer.</span>

<span class="sd">    Presented in &quot;Schaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2015. Prioritized experience replay.&quot; (https://arxiv.org/abs/1511.05952)</span>

<span class="sd">    Args:</span>
<span class="sd">        max_capacity (int): maximum capacity of the buffer.</span>
<span class="sd">        alpha (:obj:`float`): exponent α determines how much prioritization is used,</span>
<span class="sd">            with α = 0 corresponding to the uniform case.</span>
<span class="sd">        beta (:obj:`float`): importance sampling negative exponent.</span>
<span class="sd">        eps (:obj:`float`, optional): delta added to the priorities to ensure that the buffer</span>
<span class="sd">            does not contain null priorities. Defaults to 1e-8.</span>
<span class="sd">        reduction (str, optional): the reduction method for multidimensional</span>
<span class="sd">            tensordicts (ie stored trajectory). Can be one of &quot;max&quot;, &quot;min&quot;,</span>
<span class="sd">            &quot;median&quot; or &quot;mean&quot;.</span>
<span class="sd">        max_priority_within_buffer (bool, optional): if ``True``, the max-priority</span>
<span class="sd">            is tracked within the buffer. When ``False``, the max-priority tracks</span>
<span class="sd">            the maximum value since the instantiation of the sampler.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.replay_buffers import ReplayBuffer, LazyTensorStorage, PrioritizedSampler</span>
<span class="sd">        &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">        &gt;&gt;&gt; rb = ReplayBuffer(storage=LazyTensorStorage(10), sampler=PrioritizedSampler(max_capacity=10, alpha=1.0, beta=1.0))</span>
<span class="sd">        &gt;&gt;&gt; priority = torch.tensor([0, 1000])</span>
<span class="sd">        &gt;&gt;&gt; data_0 = TensorDict({&quot;reward&quot;: 0, &quot;obs&quot;: [0], &quot;action&quot;: [0], &quot;priority&quot;: priority[0]}, [])</span>
<span class="sd">        &gt;&gt;&gt; data_1 = TensorDict({&quot;reward&quot;: 1, &quot;obs&quot;: [1], &quot;action&quot;: [2], &quot;priority&quot;: priority[1]}, [])</span>
<span class="sd">        &gt;&gt;&gt; rb.add(data_0)</span>
<span class="sd">        &gt;&gt;&gt; rb.add(data_1)</span>
<span class="sd">        &gt;&gt;&gt; rb.update_priority(torch.tensor([0, 1]), priority=priority)</span>
<span class="sd">        &gt;&gt;&gt; sample, info = rb.sample(10, return_info=True)</span>
<span class="sd">        &gt;&gt;&gt; print(sample)</span>
<span class="sd">        TensorDict(</span>
<span class="sd">                fields={</span>
<span class="sd">                    action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),</span>
<span class="sd">                    obs: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),</span>
<span class="sd">                    priority: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False),</span>
<span class="sd">                    reward: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},</span>
<span class="sd">                batch_size=torch.Size([10]),</span>
<span class="sd">                device=cpu,</span>
<span class="sd">                is_shared=False)</span>
<span class="sd">        &gt;&gt;&gt; print(info)</span>
<span class="sd">        {&#39;_weight&#39;: array([1.e-11, 1.e-11, 1.e-11, 1.e-11, 1.e-11, 1.e-11, 1.e-11, 1.e-11,</span>
<span class="sd">               1.e-11, 1.e-11], dtype=float32), &#39;index&#39;: array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}</span>

<span class="sd">    .. note:: Using a :class:`~torchrl.data.replay_buffers.TensorDictReplayBuffer` can smoothen the</span>
<span class="sd">        process of updating the priorities:</span>

<span class="sd">            &gt;&gt;&gt; from torchrl.data.replay_buffers import TensorDictReplayBuffer as TDRB, LazyTensorStorage, PrioritizedSampler</span>
<span class="sd">            &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">            &gt;&gt;&gt; rb = TDRB(</span>
<span class="sd">            ...     storage=LazyTensorStorage(10),</span>
<span class="sd">            ...     sampler=PrioritizedSampler(max_capacity=10, alpha=1.0, beta=1.0),</span>
<span class="sd">            ...     priority_key=&quot;priority&quot;,  # This kwarg isn&#39;t present in regular RBs</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; priority = torch.tensor([0, 1000])</span>
<span class="sd">            &gt;&gt;&gt; data_0 = TensorDict({&quot;reward&quot;: 0, &quot;obs&quot;: [0], &quot;action&quot;: [0], &quot;priority&quot;: priority[0]}, [])</span>
<span class="sd">            &gt;&gt;&gt; data_1 = TensorDict({&quot;reward&quot;: 1, &quot;obs&quot;: [1], &quot;action&quot;: [2], &quot;priority&quot;: priority[1]}, [])</span>
<span class="sd">            &gt;&gt;&gt; data = torch.stack([data_0, data_1])</span>
<span class="sd">            &gt;&gt;&gt; rb.extend(data)</span>
<span class="sd">            &gt;&gt;&gt; rb.update_priority(data)  # Reads the &quot;priority&quot; key as indicated in the constructor</span>
<span class="sd">            &gt;&gt;&gt; sample, info = rb.sample(10, return_info=True)</span>
<span class="sd">            &gt;&gt;&gt; print(sample[&#39;index&#39;])  # The index is packed with the tensordict</span>
<span class="sd">            tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">max_capacity</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;max&quot;</span><span class="p">,</span>
        <span class="n">max_priority_within_buffer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">alpha</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;alpha must be greater or equal than 0, got alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">beta</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;beta must be greater or equal to 0, got beta=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span> <span class="o">=</span> <span class="n">max_capacity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority_within_buffer</span> <span class="o">=</span> <span class="n">max_priority_within_buffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(alpha=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="si">}</span><span class="s2">, beta=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_beta</span><span class="si">}</span><span class="s2">, eps=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_eps</span><span class="si">}</span><span class="s2">, reduction=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">max_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">alpha</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span>

    <span class="nd">@alpha</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">alpha</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">beta</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span>

    <span class="nd">@beta</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">beta</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">get_spawning_popen</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Samplers of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2"> cannot be shared between processes.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__getstate__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatType</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span> <span class="o">=</span> <span class="n">SumSegmentTreeFp32</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span> <span class="o">=</span> <span class="n">MinSegmentTreeFp32</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span> <span class="o">=</span> <span class="n">SumSegmentTreeFp64</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span> <span class="o">=</span> <span class="n">MinSegmentTreeFp64</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;dtype </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> not supported by PrioritizedSampler&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_max_priority</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">max_priority_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_max_priority&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">max_priority_index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">max_priority_index</span>

    <span class="nd">@_max_priority</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">_max_priority</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;_max_priority&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_maybe_erase_max_priority</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority_within_buffer</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">max_priority_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">max_priority_index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">def</span> <span class="nf">check_index</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="p">,</span> <span class="n">max_priority_index</span><span class="o">=</span><span class="n">max_priority_index</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="c1"># index can be 1d or 2d</span>
                <span class="k">if</span> <span class="n">index</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">is_overwritten</span> <span class="o">=</span> <span class="p">(</span><span class="n">index</span> <span class="o">==</span> <span class="n">max_priority_index</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">is_overwritten</span> <span class="o">=</span> <span class="p">(</span><span class="n">index</span> <span class="o">==</span> <span class="n">max_priority_index</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">is_overwritten</span> <span class="o">=</span> <span class="n">index</span> <span class="o">==</span> <span class="n">max_priority_index</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">slice</span><span class="p">):</span>
                <span class="c1"># This won&#39;t work if called recursively</span>
                <span class="n">is_overwritten</span> <span class="o">=</span> <span class="n">max_priority_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span>
                    <span class="n">index</span><span class="o">.</span><span class="n">indices</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">is_overwritten</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_priority_index</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">is_overwritten</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">mpi</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">max_priority_index</span><span class="p">):</span>
                        <span class="n">is_overwritten</span> <span class="o">&amp;=</span> <span class="n">check_index</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">mpi</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;index of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">index</span><span class="p">)</span><span class="si">}</span><span class="s2"> is not recognized.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">is_overwritten</span>

        <span class="n">is_overwritten</span> <span class="o">=</span> <span class="n">check_index</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">is_overwritten</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">default_priority</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="n">mp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">mp</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mp</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">mp</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span><span class="p">)</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">_EMPTY_STORAGE_ERROR</span><span class="p">)</span>
        <span class="n">p_sum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">))</span>
        <span class="n">p_min</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">p_sum</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;non-positive p_sum&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p_min</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;non-positive p_min&quot;</span><span class="p">)</span>
        <span class="c1"># For some undefined reason, only np.random works here.</span>
        <span class="c1"># All PT attempts fail, even when subsequently transformed into numpy</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mass</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">p_sum</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mass</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="p">)</span> <span class="o">*</span> <span class="n">p_sum</span>

        <span class="c1"># mass = torch.zeros(batch_size, dtype=torch.double).uniform_(0.0, p_sum)</span>
        <span class="c1"># mass = torch.rand(batch_size).mul_(p_sum)</span>
        <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="o">.</span><span class="n">scan_lower_bound</span><span class="p">(</span><span class="n">mass</span><span class="p">)</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">index</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">index</span><span class="o">.</span><span class="n">clamp_max_</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
        <span class="c1"># get indices where weight is 0</span>
        <span class="n">zero_weight</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">index</span>
        <span class="k">while</span> <span class="n">zero_weight</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">zero_weight</span><span class="p">,</span> <span class="n">index</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Failed to find a suitable index&quot;</span><span class="p">)</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
            <span class="n">zero_weight</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="c1"># Importance sampling weight formula:</span>
        <span class="c1">#   w_i = (p_i / sum(p) * N) ^ (-beta)</span>
        <span class="c1">#   weight_i = w_i / max(w)</span>
        <span class="c1">#   weight_i = (p_i / sum(p) * N) ^ (-beta) /</span>
        <span class="c1">#       ((min(p) / sum(p) * N) ^ (-beta))</span>
        <span class="c1">#   weight_i = ((p_i / sum(p) * N) / (min(p) / sum(p) * N)) ^ (-beta)</span>
        <span class="c1">#   weight_i = (p_i / min(p)) ^ (-beta)</span>
        <span class="c1"># weight = np.power(weight / (p_min + self._eps), -self._beta)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">weight</span> <span class="o">/</span> <span class="n">p_min</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_beta</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">storage</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">unravel_index</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">storage</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">index</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;_weight&quot;</span><span class="p">:</span> <span class="n">weight</span><span class="p">}</span>

        <span class="k">return</span> <span class="n">index</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;_weight&quot;</span><span class="p">:</span> <span class="n">weight</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_erase_max_priority</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_erase_max_priority</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>

<div class="viewcode-block" id="PrioritizedSampler.update_priority"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.PrioritizedSampler.html#torchrl.data.replay_buffers.PrioritizedSampler.update_priority">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">update_priority</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">index</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">priority</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">storage</span><span class="p">:</span> <span class="n">TensorStorage</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># noqa: D417</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Updates the priority of the data pointed by the index.</span>

<span class="sd">        Args:</span>
<span class="sd">            index (int or torch.Tensor): indexes of the priorities to be</span>
<span class="sd">                updated.</span>
<span class="sd">            priority (Number or torch.Tensor): new priorities of the</span>
<span class="sd">                indexed elements.</span>

<span class="sd">        Keyword Args:</span>
<span class="sd">            storage (Storage, optional): a storage used to map the Nd index size to</span>
<span class="sd">                the 1d size of the sum_tree and min_tree. Only required whenever</span>
<span class="sd">                ``index.ndim &gt; 2``.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">priority</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">priority</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
        <span class="c1"># we need to reshape priority if it has more than one element or if it has</span>
        <span class="c1"># a different shape than index</span>
        <span class="k">if</span> <span class="n">priority</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">priority</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">priority</span> <span class="o">=</span> <span class="n">priority</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;priority should be a number or an iterable of the same &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;length as index. Got priority of shape </span><span class="si">{</span><span class="n">priority</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> and index &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span> <span class="kn">from</span> <span class="nn">err</span>
        <span class="k">elif</span> <span class="n">priority</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">priority</span> <span class="o">=</span> <span class="n">priority</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="c1"># MaxValueWriter will set -1 for items in the data that we don&#39;t want</span>
        <span class="c1"># to update. We therefore have to keep only the non-negative indices.</span>
        <span class="k">if</span> <span class="n">_is_int</span><span class="p">(</span><span class="n">index</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">index</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">storage</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;storage should be provided to Sampler.update_priority when the storage has more &quot;</span>
                        <span class="s2">&quot;than one dimension.&quot;</span>
                    <span class="p">)</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">shape</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">shape</span>
                <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                        <span class="s2">&quot;Could not retrieve the storage shape. If your storage is not a TensorStorage subclass &quot;</span>
                        <span class="s2">&quot;or its shape isn&#39;t accessible via the shape attribute, submit an issue on GitHub.&quot;</span>
                    <span class="p">)</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ravel_multi_index</span><span class="p">(</span><span class="n">index</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">shape</span><span class="p">))</span>
            <span class="n">valid_index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">&gt;=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">valid_index</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="k">return</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">valid_index</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">[</span><span class="n">valid_index</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">priority</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
                    <span class="n">priority</span> <span class="o">=</span> <span class="n">priority</span><span class="p">[</span><span class="n">valid_index</span><span class="p">]</span>

        <span class="n">max_p</span><span class="p">,</span> <span class="n">max_p_idx</span> <span class="o">=</span> <span class="n">priority</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">cur_max_priority</span><span class="p">,</span> <span class="n">cur_max_priority_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span>
        <span class="k">if</span> <span class="n">cur_max_priority</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">max_p</span> <span class="o">&gt;</span> <span class="n">cur_max_priority</span><span class="p">:</span>
            <span class="n">cur_max_priority</span><span class="p">,</span> <span class="n">cur_max_priority_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">max_p</span><span class="p">,</span>
                <span class="n">index</span><span class="p">[</span><span class="n">max_p_idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">index</span><span class="o">.</span><span class="n">ndim</span> <span class="k">else</span> <span class="n">index</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">priority</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">priority</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">priority</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">priority</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority_within_buffer</span>
            <span class="ow">and</span> <span class="n">cur_max_priority_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="p">(</span><span class="n">index</span> <span class="o">==</span> <span class="n">cur_max_priority_index</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="n">maxval</span><span class="p">,</span> <span class="n">maxidx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">)]</span>
            <span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span> <span class="o">=</span> <span class="p">(</span><span class="n">maxval</span><span class="p">,</span> <span class="n">maxidx</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">mark_update</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="o">*</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_priority</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_priority</span><span class="p">,</span> <span class="n">storage</span><span class="o">=</span><span class="n">storage</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;_alpha&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">,</span>
            <span class="s2">&quot;_beta&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span><span class="p">,</span>
            <span class="s2">&quot;_eps&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span><span class="p">,</span>
            <span class="s2">&quot;_max_priority&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span><span class="p">,</span>
            <span class="s2">&quot;_sum_tree&quot;</span><span class="p">:</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">),</span>
            <span class="s2">&quot;_min_tree&quot;</span><span class="p">:</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span><span class="p">),</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_alpha&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_beta&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_eps&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_max_priority&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_sum_tree&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_min_tree&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">absolute</span><span class="p">()</span>
        <span class="n">path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">mm_st</span> <span class="o">=</span> <span class="n">MemoryMappedTensor</span><span class="o">.</span><span class="n">from_filename</span><span class="p">(</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">,),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                <span class="n">filename</span><span class="o">=</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;sumtree.memmap&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">mm_mt</span> <span class="o">=</span> <span class="n">MemoryMappedTensor</span><span class="o">.</span><span class="n">from_filename</span><span class="p">(</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">,),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                <span class="n">filename</span><span class="o">=</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;mintree.memmap&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
            <span class="n">mm_st</span> <span class="o">=</span> <span class="n">MemoryMappedTensor</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">,),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                <span class="n">filename</span><span class="o">=</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;sumtree.memmap&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">mm_mt</span> <span class="o">=</span> <span class="n">MemoryMappedTensor</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">,),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                <span class="n">filename</span><span class="o">=</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;mintree.memmap&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">mm_st</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">)])</span>
        <span class="p">)</span>
        <span class="n">mm_mt</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">)])</span>
        <span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;sampler_metadata.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;_alpha&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">,</span>
                    <span class="s2">&quot;_beta&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span><span class="p">,</span>
                    <span class="s2">&quot;_eps&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span><span class="p">,</span>
                    <span class="s2">&quot;_max_priority&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span><span class="p">,</span>
                    <span class="s2">&quot;_max_capacity&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">,</span>
                <span class="p">},</span>
                <span class="n">file</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">absolute</span><span class="p">()</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;sampler_metadata.json&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">metadata</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;_alpha&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;_beta&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;_eps&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;_max_priority&quot;</span><span class="p">]</span>
        <span class="n">_max_capacity</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;_max_capacity&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">_max_capacity</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;max capacity of loaded metadata (</span><span class="si">{</span><span class="n">_max_capacity</span><span class="si">}</span><span class="s2">) differs from self._max_capacity (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="si">}</span><span class="s2">).&quot;</span>
            <span class="p">)</span>
        <span class="n">mm_st</span> <span class="o">=</span> <span class="n">MemoryMappedTensor</span><span class="o">.</span><span class="n">from_filename</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">,),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
            <span class="n">filename</span><span class="o">=</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;sumtree.memmap&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">mm_mt</span> <span class="o">=</span> <span class="n">MemoryMappedTensor</span><span class="o">.</span><span class="n">from_filename</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">,),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
            <span class="n">filename</span><span class="o">=</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;mintree.memmap&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">elt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mm_st</span><span class="o">.</span><span class="n">tolist</span><span class="p">()):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">elt</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">elt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mm_mt</span><span class="o">.</span><span class="n">tolist</span><span class="p">()):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">elt</span></div>


<div class="viewcode-block" id="SliceSampler"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.SliceSampler.html#torchrl.data.replay_buffers.SliceSampler">[docs]</a><span class="k">class</span> <span class="nc">SliceSampler</span><span class="p">(</span><span class="n">Sampler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Samples slices of data along the first dimension, given start and stop signals.</span>

<span class="sd">    This class samples sub-trajectories with replacement. For a version without</span>
<span class="sd">    replacement, see :class:`~torchrl.data.replay_buffers.samplers.SliceSamplerWithoutReplacement`.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        num_slices (int): the number of slices to be sampled. The batch-size</span>
<span class="sd">            must be greater or equal to the ``num_slices`` argument. Exclusive</span>
<span class="sd">            with ``slice_len``.</span>
<span class="sd">        slice_len (int): the length of the slices to be sampled. The batch-size</span>
<span class="sd">            must be greater or equal to the ``slice_len`` argument and divisible</span>
<span class="sd">            by it. Exclusive with ``num_slices``.</span>
<span class="sd">        end_key (NestedKey, optional): the key indicating the end of a</span>
<span class="sd">            trajectory (or episode). Defaults to ``(&quot;next&quot;, &quot;done&quot;)``.</span>
<span class="sd">        traj_key (NestedKey, optional): the key indicating the trajectories.</span>
<span class="sd">            Defaults to ``&quot;episode&quot;`` (commonly used across datasets in TorchRL).</span>
<span class="sd">        ends (torch.Tensor, optional): a 1d boolean tensor containing the end of run signals.</span>
<span class="sd">            To be used whenever the ``end_key`` or ``traj_key`` is expensive to get,</span>
<span class="sd">            or when this signal is readily available. Must be used with ``cache_values=True``</span>
<span class="sd">            and cannot be used in conjunction with ``end_key`` or ``traj_key``.</span>
<span class="sd">            If provided, it is assumed that the storage is at capacity and that</span>
<span class="sd">            if the last element of the ``ends`` tensor is ``False``,</span>
<span class="sd">            the same trajectory spans across end and beginning.</span>
<span class="sd">        trajectories (torch.Tensor, optional): a 1d integer tensor containing the run ids.</span>
<span class="sd">            To be used whenever the ``end_key`` or ``traj_key`` is expensive to get,</span>
<span class="sd">            or when this signal is readily available. Must be used with ``cache_values=True``</span>
<span class="sd">            and cannot be used in conjunction with ``end_key`` or ``traj_key``.</span>
<span class="sd">            If provided, it is assumed that the storage is at capacity and that</span>
<span class="sd">            if the last element of the trajectory tensor is identical to the first,</span>
<span class="sd">            the same trajectory spans across end and beginning.</span>
<span class="sd">        cache_values (bool, optional): to be used with static datasets.</span>
<span class="sd">            Will cache the start and end signal of the trajectory. This can be safely used even</span>
<span class="sd">            if the trajectory indices change during calls to :class:`~torchrl.data.ReplayBuffer.extend`</span>
<span class="sd">            as this operation will erase the cache.</span>

<span class="sd">            .. warning:: ``cache_values=True`` will not work if the sampler is used with a</span>
<span class="sd">                storage that is extended by another buffer. For instance:</span>

<span class="sd">                    &gt;&gt;&gt; buffer0 = ReplayBuffer(storage=storage,</span>
<span class="sd">                    ...     sampler=SliceSampler(num_slices=8, cache_values=True),</span>
<span class="sd">                    ...     writer=ImmutableWriter())</span>
<span class="sd">                    &gt;&gt;&gt; buffer1 = ReplayBuffer(storage=storage,</span>
<span class="sd">                    ...     sampler=other_sampler)</span>
<span class="sd">                    &gt;&gt;&gt; # Wrong! Does not erase the buffer from the sampler of buffer0</span>
<span class="sd">                    &gt;&gt;&gt; buffer1.extend(data)</span>

<span class="sd">            .. warning:: ``cache_values=True`` will not work as expected if the buffer is</span>
<span class="sd">                shared between processes and one process is responsible for writing</span>
<span class="sd">                and one process for sampling, as erasing the cache can only be done locally.</span>

<span class="sd">        truncated_key (NestedKey, optional): If not ``None``, this argument</span>
<span class="sd">            indicates where a truncated signal should be written in the output</span>
<span class="sd">            data. This is used to indicate to value estimators where the provided</span>
<span class="sd">            trajectory breaks. Defaults to ``(&quot;next&quot;, &quot;truncated&quot;)``.</span>
<span class="sd">            This feature only works with :class:`~torchrl.data.replay_buffers.TensorDictReplayBuffer`</span>
<span class="sd">            instances (otherwise the truncated key is returned in the info dictionary</span>
<span class="sd">            returned by the :meth:`~torchrl.data.replay_buffers.ReplayBuffer.sample` method).</span>
<span class="sd">        strict_length (bool, optional): if ``False``, trajectories of length</span>
<span class="sd">            shorter than `slice_len` (or `batch_size // num_slices`) will be</span>
<span class="sd">            allowed to appear in the batch. If ``True``, trajectories shorted</span>
<span class="sd">            than required will be filtered out.</span>
<span class="sd">            Be mindful that this can result in effective `batch_size`  shorter</span>
<span class="sd">            than the one asked for! Trajectories can be split using</span>
<span class="sd">            :func:`~torchrl.collectors.split_trajectories`. Defaults to ``True``.</span>
<span class="sd">        compile (bool or dict of kwargs, optional): if ``True``, the bottleneck of</span>
<span class="sd">            the :meth:`~sample` method will be compiled with :func:`~torch.compile`.</span>
<span class="sd">            Keyword arguments can also be passed to torch.compile with this arg.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        span (bool, int, Tuple[bool | int, bool | int], optional): if provided, the sampled</span>
<span class="sd">            trajectory will span across the left and/or the right. This means that possibly</span>
<span class="sd">            fewer elements will be provided than what was required. A boolean value means</span>
<span class="sd">            that at least one element will be sampled per trajectory. An integer `i` means</span>
<span class="sd">            that at least `slice_len - i` samples will be gathered for each sampled trajectory.</span>
<span class="sd">            Using tuples allows a fine grained control over the span on the left (beginning</span>
<span class="sd">            of the stored trajectory) and on the right (end of the stored trajectory).</span>

<span class="sd">    .. note:: To recover the trajectory splits in the storage,</span>
<span class="sd">        :class:`~torchrl.data.replay_buffers.samplers.SliceSampler` will first</span>
<span class="sd">        attempt to find the ``traj_key`` entry in the storage. If it cannot be</span>
<span class="sd">        found, the ``end_key`` will be used to reconstruct the episodes.</span>

<span class="sd">    .. note:: When using `strict_length=False`, it is recommended to use</span>
<span class="sd">        :func:`~torchrl.collectors.utils.split_trajectories` to split the sampled trajectories.</span>
<span class="sd">        However, if two samples from the same episode are placed next to each other,</span>
<span class="sd">        this may produce incorrect results. To avoid this issue, consider one of these solutions:</span>

<span class="sd">        - using a :class:`~torchrl.data.TensorDictReplayBuffer` instance with the slice sampler</span>

<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">            &gt;&gt;&gt; from torchrl.collectors.utils import split_trajectories</span>
<span class="sd">            &gt;&gt;&gt; from torchrl.data import TensorDictReplayBuffer, ReplayBuffer, LazyTensorStorage, SliceSampler, SliceSamplerWithoutReplacement</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; rb = TensorDictReplayBuffer(storage=LazyTensorStorage(max_size=1000),</span>
<span class="sd">            ...                   sampler=SliceSampler(</span>
<span class="sd">            ...                       slice_len=5, traj_key=&quot;episode&quot;,strict_length=False,</span>
<span class="sd">            ...                   ))</span>
<span class="sd">            ...</span>
<span class="sd">            &gt;&gt;&gt; ep_1 = TensorDict(</span>
<span class="sd">            ...     {&quot;obs&quot;: torch.arange(100),</span>
<span class="sd">            ...     &quot;episode&quot;: torch.zeros(100),},</span>
<span class="sd">            ...     batch_size=[100]</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; ep_2 = TensorDict(</span>
<span class="sd">            ...     {&quot;obs&quot;: torch.arange(4),</span>
<span class="sd">            ...     &quot;episode&quot;: torch.ones(4),},</span>
<span class="sd">            ...     batch_size=[4]</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; rb.extend(ep_1)</span>
<span class="sd">            &gt;&gt;&gt; rb.extend(ep_2)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; s = rb.sample(50)</span>
<span class="sd">            &gt;&gt;&gt; print(s)</span>
<span class="sd">            TensorDict(</span>
<span class="sd">                fields={</span>
<span class="sd">                    episode: Tensor(shape=torch.Size([46]), device=cpu, dtype=torch.float32, is_shared=False),</span>
<span class="sd">                    index: Tensor(shape=torch.Size([46, 1]), device=cpu, dtype=torch.int64, is_shared=False),</span>
<span class="sd">                    next: TensorDict(</span>
<span class="sd">                        fields={</span>
<span class="sd">                            done: Tensor(shape=torch.Size([46, 1]), device=cpu, dtype=torch.bool, is_shared=False),</span>
<span class="sd">                            terminated: Tensor(shape=torch.Size([46, 1]), device=cpu, dtype=torch.bool, is_shared=False),</span>
<span class="sd">                            truncated: Tensor(shape=torch.Size([46, 1]), device=cpu, dtype=torch.bool, is_shared=False)},</span>
<span class="sd">                        batch_size=torch.Size([46]),</span>
<span class="sd">                        device=cpu,</span>
<span class="sd">                        is_shared=False),</span>
<span class="sd">                    obs: Tensor(shape=torch.Size([46]), device=cpu, dtype=torch.int64, is_shared=False)},</span>
<span class="sd">                batch_size=torch.Size([46]),</span>
<span class="sd">                device=cpu,</span>
<span class="sd">                is_shared=False)</span>
<span class="sd">            &gt;&gt;&gt; t = split_trajectories(s, done_key=&quot;truncated&quot;)</span>
<span class="sd">            &gt;&gt;&gt; print(t[&quot;obs&quot;])</span>
<span class="sd">            tensor([[73, 74, 75, 76, 77],</span>
<span class="sd">                    [ 0,  1,  2,  3,  0],</span>
<span class="sd">                    [ 0,  1,  2,  3,  0],</span>
<span class="sd">                    [41, 42, 43, 44, 45],</span>
<span class="sd">                    [ 0,  1,  2,  3,  0],</span>
<span class="sd">                    [67, 68, 69, 70, 71],</span>
<span class="sd">                    [27, 28, 29, 30, 31],</span>
<span class="sd">                    [80, 81, 82, 83, 84],</span>
<span class="sd">                    [17, 18, 19, 20, 21],</span>
<span class="sd">                    [ 0,  1,  2,  3,  0]])</span>
<span class="sd">            &gt;&gt;&gt; print(t[&quot;episode&quot;])</span>
<span class="sd">            tensor([[0., 0., 0., 0., 0.],</span>
<span class="sd">                    [1., 1., 1., 1., 0.],</span>
<span class="sd">                    [1., 1., 1., 1., 0.],</span>
<span class="sd">                    [0., 0., 0., 0., 0.],</span>
<span class="sd">                    [1., 1., 1., 1., 0.],</span>
<span class="sd">                    [0., 0., 0., 0., 0.],</span>
<span class="sd">                    [0., 0., 0., 0., 0.],</span>
<span class="sd">                    [0., 0., 0., 0., 0.],</span>
<span class="sd">                    [0., 0., 0., 0., 0.],</span>
<span class="sd">                    [1., 1., 1., 1., 0.]])</span>

<span class="sd">        - using a :class:`~torchrl.data.replay_buffers.samplers.SliceSamplerWithoutReplacement`</span>

<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">            &gt;&gt;&gt; from torchrl.collectors.utils import split_trajectories</span>
<span class="sd">            &gt;&gt;&gt; from torchrl.data import ReplayBuffer, LazyTensorStorage, SliceSampler, SliceSamplerWithoutReplacement</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; rb = ReplayBuffer(storage=LazyTensorStorage(max_size=1000),</span>
<span class="sd">            ...                   sampler=SliceSamplerWithoutReplacement(</span>
<span class="sd">            ...                       slice_len=5, traj_key=&quot;episode&quot;,strict_length=False</span>
<span class="sd">            ...                   ))</span>
<span class="sd">            ...</span>
<span class="sd">            &gt;&gt;&gt; ep_1 = TensorDict(</span>
<span class="sd">            ...     {&quot;obs&quot;: torch.arange(100),</span>
<span class="sd">            ...     &quot;episode&quot;: torch.zeros(100),},</span>
<span class="sd">            ...     batch_size=[100]</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; ep_2 = TensorDict(</span>
<span class="sd">            ...     {&quot;obs&quot;: torch.arange(4),</span>
<span class="sd">            ...     &quot;episode&quot;: torch.ones(4),},</span>
<span class="sd">            ...     batch_size=[4]</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; rb.extend(ep_1)</span>
<span class="sd">            &gt;&gt;&gt; rb.extend(ep_2)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; s = rb.sample(50)</span>
<span class="sd">            &gt;&gt;&gt; t = split_trajectories(s, trajectory_key=&quot;episode&quot;)</span>
<span class="sd">            &gt;&gt;&gt; print(t[&quot;obs&quot;])</span>
<span class="sd">            tensor([[75, 76, 77, 78, 79],</span>
<span class="sd">                    [ 0,  1,  2,  3,  0]])</span>
<span class="sd">            &gt;&gt;&gt; print(t[&quot;episode&quot;])</span>
<span class="sd">            tensor([[0., 0., 0., 0., 0.],</span>
<span class="sd">                    [1., 1., 1., 1., 0.]])</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.replay_buffers import LazyMemmapStorage, TensorDictReplayBuffer</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.replay_buffers.samplers import SliceSampler</span>
<span class="sd">        &gt;&gt;&gt; torch.manual_seed(0)</span>
<span class="sd">        &gt;&gt;&gt; rb = TensorDictReplayBuffer(</span>
<span class="sd">        ...     storage=LazyMemmapStorage(1_000_000),</span>
<span class="sd">        ...     sampler=SliceSampler(cache_values=True, num_slices=10),</span>
<span class="sd">        ...     batch_size=320,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; episode = torch.zeros(1000, dtype=torch.int)</span>
<span class="sd">        &gt;&gt;&gt; episode[:300] = 1</span>
<span class="sd">        &gt;&gt;&gt; episode[300:550] = 2</span>
<span class="sd">        &gt;&gt;&gt; episode[550:700] = 3</span>
<span class="sd">        &gt;&gt;&gt; episode[700:] = 4</span>
<span class="sd">        &gt;&gt;&gt; data = TensorDict(</span>
<span class="sd">        ...     {</span>
<span class="sd">        ...         &quot;episode&quot;: episode,</span>
<span class="sd">        ...         &quot;obs&quot;: torch.randn((3, 4, 5)).expand(1000, 3, 4, 5),</span>
<span class="sd">        ...         &quot;act&quot;: torch.randn((20,)).expand(1000, 20),</span>
<span class="sd">        ...         &quot;other&quot;: torch.randn((20, 50)).expand(1000, 20, 50),</span>
<span class="sd">        ...     }, [1000]</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; rb.extend(data)</span>
<span class="sd">        &gt;&gt;&gt; sample = rb.sample()</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;sample:&quot;, sample)</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;episodes&quot;, sample.get(&quot;episode&quot;).unique())</span>
<span class="sd">        episodes tensor([1, 2, 3, 4], dtype=torch.int32)</span>

<span class="sd">    :class:`~torchrl.data.replay_buffers.SliceSampler` is default-compatible with</span>
<span class="sd">    most of TorchRL&#39;s datasets:</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.datasets import RobosetExperienceReplay</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data import SliceSampler</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; torch.manual_seed(0)</span>
<span class="sd">        &gt;&gt;&gt; num_slices = 10</span>
<span class="sd">        &gt;&gt;&gt; dataid = list(RobosetExperienceReplay.available_datasets)[0]</span>
<span class="sd">        &gt;&gt;&gt; data = RobosetExperienceReplay(dataid, batch_size=320, sampler=SliceSampler(num_slices=num_slices))</span>
<span class="sd">        &gt;&gt;&gt; for batch in data:</span>
<span class="sd">        ...     batch = batch.reshape(num_slices, -1)</span>
<span class="sd">        ...     break</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;check that each batch only has one episode:&quot;, batch[&quot;episode&quot;].unique(dim=1))</span>
<span class="sd">        check that each batch only has one episode: tensor([[19],</span>
<span class="sd">                [14],</span>
<span class="sd">                [ 8],</span>
<span class="sd">                [10],</span>
<span class="sd">                [13],</span>
<span class="sd">                [ 4],</span>
<span class="sd">                [ 2],</span>
<span class="sd">                [ 3],</span>
<span class="sd">                [22],</span>
<span class="sd">                [ 8]])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># We use this whenever we need to sample N times too many transitions to then select only a 1/N fraction of them</span>
    <span class="n">_batch_size_multiplier</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">slice_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">end_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">traj_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ends</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trajectories</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_values</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncated_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;truncated&quot;</span><span class="p">),</span>
        <span class="n">strict_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="nb">compile</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">span</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">bool</span> <span class="o">|</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span> <span class="o">=</span> <span class="n">num_slices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span> <span class="o">=</span> <span class="n">slice_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end_key</span> <span class="o">=</span> <span class="n">end_key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">traj_key</span> <span class="o">=</span> <span class="n">traj_key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span> <span class="o">=</span> <span class="n">truncated_key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_values</span> <span class="o">=</span> <span class="n">cache_values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fetch_traj</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strict_length</span> <span class="o">=</span> <span class="n">strict_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">span</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
            <span class="n">span</span> <span class="o">=</span> <span class="p">(</span><span class="n">span</span><span class="p">,</span> <span class="n">span</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">span</span> <span class="o">=</span> <span class="n">span</span>

        <span class="k">if</span> <span class="n">trajectories</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">traj_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">end_key</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;`trajectories` and `end_key` or `traj_key` are exclusive arguments.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">ends</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;trajectories and ends are exclusive arguments.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">cache_values</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;To be used, trajectories requires `cache_values` to be set to `True`.&quot;</span>
                <span class="p">)</span>
            <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_find_start_stop_traj</span><span class="p">(</span>
                <span class="n">trajectory</span><span class="o">=</span><span class="n">trajectories</span><span class="p">,</span>
                <span class="n">at_capacity</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="s2">&quot;stop-and-length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vals</span>

        <span class="k">elif</span> <span class="n">ends</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">traj_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">end_key</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;`ends` and `end_key` or `traj_key` are exclusive arguments.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">trajectories</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;trajectories and ends are exclusive arguments.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">cache_values</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;To be used, ends requires `cache_values` to be set to `True`.&quot;</span>
                <span class="p">)</span>
            <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_find_start_stop_traj</span><span class="p">(</span><span class="n">end</span><span class="o">=</span><span class="n">ends</span><span class="p">,</span> <span class="n">at_capacity</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="s2">&quot;stop-and-length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vals</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">end_key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">end_key</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;done&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">traj_key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">traj_key</span> <span class="o">=</span> <span class="s2">&quot;episode&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">end_key</span> <span class="o">=</span> <span class="n">end_key</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">traj_key</span> <span class="o">=</span> <span class="n">traj_key</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">((</span><span class="n">num_slices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span><span class="n">slice_len</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;Either num_slices or slice_len must be not None, and not both. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Got num_slices=</span><span class="si">{</span><span class="n">num_slices</span><span class="si">}</span><span class="s2"> and slice_len=</span><span class="si">{</span><span class="n">slice_len</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compile</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="nb">compile</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">compile</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">compile</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">compile</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_index</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">get_spawning_popen</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_values</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;It seems you are sharing a </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> across processes with&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;cache_values=True. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;While this isn&#39;t forbidden and could perfectly work if your dataset &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;is unaltered on both processes, remember that calling extend/add on&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;one process will NOT erase the cache on another process&#39;s sampler, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;which will cause synchronization issues.&quot;</span>
            <span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__getstate__</span><span class="p">()</span>
        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_cache&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">return</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_values</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_values</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(num_slices=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;slice_len=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;end_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">end_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;traj_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">traj_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;truncated_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;strict_length=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">strict_length</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_find_start_stop_traj</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">trajectory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">at_capacity</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">cursor</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">trajectory</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># slower</span>
            <span class="c1"># _, stop_idx = torch.unique_consecutive(trajectory, return_counts=True)</span>
            <span class="c1"># stop_idx = stop_idx.cumsum(0) - 1</span>

            <span class="c1"># even slower</span>
            <span class="c1"># t = trajectory.unsqueeze(0)</span>
            <span class="c1"># w = torch.tensor([1, -1], dtype=torch.int).view(1, 1, 2)</span>
            <span class="c1"># stop_idx = torch.conv1d(t, w).nonzero()</span>

            <span class="c1"># faster</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">trajectory</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">trajectory</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">at_capacity</span><span class="p">:</span>
                <span class="n">end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">end</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">end</span><span class="p">[:</span><span class="mi">1</span><span class="p">])],</span> <span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">end</span><span class="p">,</span> <span class="n">trajectory</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">!=</span> <span class="n">trajectory</span><span class="p">[:</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">length</span> <span class="o">=</span> <span class="n">trajectory</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># We presume that not done at the end means that the traj spans across end and beginning of storage</span>
            <span class="n">length</span> <span class="o">=</span> <span class="n">end</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">at_capacity</span><span class="p">:</span>
                <span class="n">end</span> <span class="o">=</span> <span class="n">end</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="n">end</span><span class="p">[</span><span class="n">length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="n">end</span><span class="o">.</span><span class="n">ndim</span>

        <span class="k">if</span> <span class="n">at_capacity</span><span class="p">:</span>
            <span class="c1"># we must have at least one end by traj to individuate trajectories</span>
            <span class="c1"># so if no end can be found we set it manually</span>
            <span class="k">if</span> <span class="n">cursor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cursor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">cursor</span> <span class="o">=</span> <span class="n">cursor</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cursor</span><span class="p">,</span> <span class="nb">range</span><span class="p">):</span>
                    <span class="n">cursor</span> <span class="o">=</span> <span class="n">cursor</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_int</span><span class="p">(</span><span class="n">cursor</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;cursor should be an integer or a 1d tensor or a range.&quot;</span>
                    <span class="p">)</span>
                <span class="n">end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">index_fill</span><span class="p">(</span>
                    <span class="n">end</span><span class="p">,</span>
                    <span class="n">index</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">cursor</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">end</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">),</span>
                    <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">end</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="o">~</span><span class="n">end</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">end</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">mask</span><span class="p">])</span>
                <span class="n">end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Expected the end-of-trajectory signal to be at least 1-dimensional.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_end_to_start_stop</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="n">length</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="n">end</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_end_to_start_stop</span><span class="p">(</span><span class="n">end</span><span class="p">,</span> <span class="n">length</span><span class="p">):</span>
        <span class="c1"># Using transpose ensures the start and stop are sorted the same way</span>
        <span class="n">stop_idx</span> <span class="o">=</span> <span class="n">end</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
        <span class="n">stop_idx</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">stop_idx</span><span class="p">[:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="c1"># First build the start indices as the stop + 1, we&#39;ll shift it later</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">stop_idx</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">start_idx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">start_idx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">%=</span> <span class="n">end</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># shift start: to do this, we check when the non-first dim indices are identical</span>
        <span class="c1"># and get a mask like [False, True, True, False, True, ...] where False means</span>
        <span class="c1"># that there&#39;s a switch from one dim to another (ie, a switch from one element of the batch</span>
        <span class="c1"># to another). We roll this one step along the time dimension and these two</span>
        <span class="c1"># masks provide us with the indices of the permutation matrix we need</span>
        <span class="c1"># to apply to start_idx.</span>
        <span class="k">if</span> <span class="n">start_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">start_idx_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">start_idx</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">==</span> <span class="n">start_idx</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:])</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">m1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">start_idx_mask</span><span class="p">[:</span><span class="mi">1</span><span class="p">]),</span> <span class="n">start_idx_mask</span><span class="p">])</span>
            <span class="n">m2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">start_idx_mask</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">start_idx_mask</span><span class="p">[:</span><span class="mi">1</span><span class="p">])])</span>
            <span class="n">start_idx_replace</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">start_idx</span><span class="p">)</span>
            <span class="n">start_idx_replace</span><span class="p">[</span><span class="n">m1</span><span class="p">]</span> <span class="o">=</span> <span class="n">start_idx</span><span class="p">[</span><span class="n">m2</span><span class="p">]</span>
            <span class="n">start_idx_replace</span><span class="p">[</span><span class="o">~</span><span class="n">m1</span><span class="p">]</span> <span class="o">=</span> <span class="n">start_idx</span><span class="p">[</span><span class="o">~</span><span class="n">m2</span><span class="p">]</span>
            <span class="n">start_idx</span> <span class="o">=</span> <span class="n">start_idx_replace</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># In this case we have only one start and stop has already been set</span>
            <span class="k">pass</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">stop_idx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">start_idx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">lengths</span><span class="p">[</span><span class="n">lengths</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">lengths</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">length</span>
        <span class="k">return</span> <span class="n">start_idx</span><span class="p">,</span> <span class="n">stop_idx</span><span class="p">,</span> <span class="n">lengths</span>

    <span class="k">def</span> <span class="nf">_start_to_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">st</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">length</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>

        <span class="n">arange</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">ndims</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">st</span><span class="o">.</span><span class="n">ndim</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">ndims</span><span class="p">:</span>
            <span class="n">arange</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">arange</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">arange</span><span class="p">)]</span> <span class="o">*</span> <span class="n">ndims</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">arange</span> <span class="o">=</span> <span class="n">arange</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">st</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">arange</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="c1"># we do this to make sure that we&#39;re not broadcasting the start</span>
            <span class="c1"># wrong as a tensor with shape [N] can&#39;t be expanded to [N, 1]</span>
            <span class="c1"># without getting an error</span>
            <span class="n">st</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">arange</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">arange</span> <span class="o">+</span> <span class="n">st</span>

    <span class="k">def</span> <span class="nf">_tensor_slices_from_startend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">storage_length</span><span class="p">):</span>
        <span class="c1"># start is a 2d tensor resulting from nonzero()</span>
        <span class="c1"># seq_length is a 1d tensor indicating the desired length of each sequence</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">arange</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">start</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">start</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">ndims</span> <span class="o">=</span> <span class="n">start</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="p">(</span><span class="n">start</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">ndims</span><span class="p">:</span>
                <span class="n">arange_reshaped</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                    <span class="n">arange</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">ndims</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]),</span>
                    <span class="n">device</span><span class="o">=</span><span class="n">start</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">start</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">arange_reshaped</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">arange</span>
                <span class="n">arange_reshaped</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">arange_reshaped</span> <span class="o">=</span> <span class="n">arange</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">arange_expanded</span> <span class="o">=</span> <span class="n">arange_reshaped</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">start</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span> <span class="o">+</span> <span class="n">arange_reshaped</span><span class="o">.</span><span class="n">shape</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">start</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">arange_expanded</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="n">n_missing_dims</span> <span class="o">=</span> <span class="n">arange_expanded</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
                <span class="n">start_expanded</span> <span class="o">=</span> <span class="n">start</span><span class="p">[</span>
                    <span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">),)</span> <span class="o">+</span> <span class="p">(</span><span class="kc">None</span><span class="p">,)</span> <span class="o">*</span> <span class="n">n_missing_dims</span>
                <span class="p">]</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">arange_expanded</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="p">(</span><span class="n">start_expanded</span> <span class="o">+</span> <span class="n">arange_expanded</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># when padding is needed</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_start_to_end</span><span class="p">(</span><span class="n">_start</span><span class="p">,</span> <span class="n">_seq_len</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">_start</span><span class="p">,</span> <span class="n">_seq_len</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>
                <span class="p">]</span>
            <span class="p">)</span>
        <span class="n">result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="n">storage_length</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">_get_stop_and_length</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">,</span> <span class="n">fallback</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_values</span> <span class="ow">and</span> <span class="s2">&quot;stop-and-length&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stop-and-length&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fetch_traj</span><span class="p">:</span>
            <span class="c1"># We first try with the traj_key</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">TensorStorage</span><span class="p">):</span>
                    <span class="n">trajectory</span> <span class="o">=</span> <span class="n">storage</span><span class="p">[:][</span><span class="bp">self</span><span class="o">.</span><span class="n">_used_traj_key</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">trajectory</span> <span class="o">=</span> <span class="n">storage</span><span class="p">[:][</span><span class="bp">self</span><span class="o">.</span><span class="n">traj_key</span><span class="p">]</span>
                    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="s2">&quot;Could not get a tensordict out of the storage, which is required for SliceSampler to compute the trajectories.&quot;</span>
                        <span class="p">)</span>
                <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_find_start_stop_traj</span><span class="p">(</span>
                    <span class="n">trajectory</span><span class="o">=</span><span class="n">trajectory</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span>
                    <span class="n">at_capacity</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">_is_full</span><span class="p">,</span>
                    <span class="n">cursor</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="s2">&quot;_last_cursor&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_values</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="s2">&quot;stop-and-length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vals</span>
                <span class="k">return</span> <span class="n">vals</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">fallback</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_fetch_traj</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_stop_and_length</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">fallback</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="k">raise</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">done</span> <span class="o">=</span> <span class="n">storage</span><span class="p">[:][</span><span class="bp">self</span><span class="o">.</span><span class="n">end_key</span><span class="p">]</span>
                <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;Could not get a tensordict out of the storage, which is required for SliceSampler to compute the trajectories.&quot;</span>
                    <span class="p">)</span>
                <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_find_start_stop_traj</span><span class="p">(</span>
                    <span class="n">end</span><span class="o">=</span><span class="n">done</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()[:</span> <span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">)],</span>
                    <span class="n">at_capacity</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">_is_full</span><span class="p">,</span>
                    <span class="n">cursor</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="s2">&quot;_last_cursor&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_values</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="s2">&quot;stop-and-length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vals</span>
                <span class="k">return</span> <span class="n">vals</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">fallback</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_fetch_traj</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_stop_and_length</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">fallback</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="k">raise</span>

    <span class="k">def</span> <span class="nf">_adjusted_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The batch-size must be divisible by the number of slices, got &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;batch_size=</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2"> and num_slices=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="n">seq_length</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span>
            <span class="n">num_slices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The batch-size must be divisible by the slice length, got &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;batch_size=</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2"> and slice_len=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="n">seq_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span>
            <span class="n">num_slices</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span>
        <span class="k">return</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_slices</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size_multiplier</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size_multiplier</span>
        <span class="c1"># pick up as many trajs as we need</span>
        <span class="n">start_idx</span><span class="p">,</span> <span class="n">stop_idx</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_stop_and_length</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span>
        <span class="c1"># we have to make sure that the number of dims of the storage</span>
        <span class="c1"># is the same as the stop/start signals since we will</span>
        <span class="c1"># use these to sample the storage</span>
        <span class="k">if</span> <span class="n">start_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">storage</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected the end-of-trajectory signal to be &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">storage</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">-dimensional. Got a </span><span class="si">{</span><span class="n">start_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> tensor &quot;</span>
                <span class="s2">&quot;instead.&quot;</span>
            <span class="p">)</span>
        <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_slices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_adjusted_batch_size</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">storage_length</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_slices</span><span class="p">(</span>
            <span class="n">lengths</span><span class="p">,</span>
            <span class="n">start_idx</span><span class="p">,</span>
            <span class="n">stop_idx</span><span class="p">,</span>
            <span class="n">seq_length</span><span class="p">,</span>
            <span class="n">num_slices</span><span class="p">,</span>
            <span class="n">storage_length</span><span class="o">=</span><span class="n">storage_length</span><span class="p">,</span>
            <span class="n">storage</span><span class="o">=</span><span class="n">storage</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sample_slices</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">start_idx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">stop_idx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">seq_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">storage_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">traj_idx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">storage</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
        <span class="c1"># start_idx and stop_idx are 2d tensors organized like a non-zero</span>

        <span class="k">def</span> <span class="nf">get_traj_idx</span><span class="p">(</span><span class="n">maxval</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span>
                <span class="n">maxval</span><span class="p">,</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">lengths</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rng</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">lengths</span> <span class="o">&lt;</span> <span class="n">seq_length</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">strict_length</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">lengths</span> <span class="o">&gt;=</span> <span class="n">seq_length</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">idx</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Did not find a single trajectory with sufficient length (length range: </span><span class="si">{</span><span class="n">lengths</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">}</span><span class="s2"> - </span><span class="si">{</span><span class="n">lengths</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">}</span><span class="s2"> / required=</span><span class="si">{</span><span class="n">seq_length</span><span class="si">}</span><span class="s2">)).&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="n">seq_length</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">lengths</span><span class="o">.</span><span class="n">shape</span>
                <span class="p">):</span>
                    <span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">lengths_idx</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">start_idx</span> <span class="o">=</span> <span class="n">start_idx</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">stop_idx</span> <span class="o">=</span> <span class="n">stop_idx</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">traj_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">traj_idx</span> <span class="o">=</span> <span class="n">get_traj_idx</span><span class="p">(</span><span class="n">lengths_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Here we must filter out the indices that correspond to trajectories</span>
                    <span class="c1"># we don&#39;t want to keep. That could potentially lead to an empty sample.</span>
                    <span class="c1"># The difficulty with this adjustment is that traj_idx points to a full</span>
                    <span class="c1"># sequences of lengths, but we filter out part of it so we must</span>
                    <span class="c1"># convert traj_idx to a boolean mask, index this mask with the</span>
                    <span class="c1"># valid indices and then recover the nonzero.</span>
                    <span class="n">idx_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
                    <span class="n">idx_mask</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">traj_idx</span> <span class="o">=</span> <span class="n">idx_mask</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">traj_idx</span><span class="o">.</span><span class="n">numel</span><span class="p">():</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="s2">&quot;None of the provided indices pointed to a trajectory of &quot;</span>
                            <span class="s2">&quot;sufficient length. Consider using strict_length=False for the &quot;</span>
                            <span class="s2">&quot;sampler instead.&quot;</span>
                        <span class="p">)</span>
                    <span class="n">num_slices</span> <span class="o">=</span> <span class="n">traj_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="k">del</span> <span class="n">idx</span>
                <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths_idx</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">traj_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">traj_idx</span> <span class="o">=</span> <span class="n">get_traj_idx</span><span class="p">(</span><span class="n">lengths</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">num_slices</span> <span class="o">=</span> <span class="n">traj_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="c1"># make seq_length a tensor with values clamped by lengths</span>
                <span class="n">seq_length</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span><span class="o">.</span><span class="n">clamp_max</span><span class="p">(</span><span class="n">seq_length</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">traj_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">traj_idx</span> <span class="o">=</span> <span class="n">get_traj_idx</span><span class="p">(</span><span class="n">lengths</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">num_slices</span> <span class="o">=</span> <span class="n">traj_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_index</span><span class="p">(</span>
            <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">,</span>
            <span class="n">start_idx</span><span class="o">=</span><span class="n">start_idx</span><span class="p">,</span>
            <span class="n">stop_idx</span><span class="o">=</span><span class="n">stop_idx</span><span class="p">,</span>
            <span class="n">num_slices</span><span class="o">=</span><span class="n">num_slices</span><span class="p">,</span>
            <span class="n">seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
            <span class="n">storage_length</span><span class="o">=</span><span class="n">storage_length</span><span class="p">,</span>
            <span class="n">traj_idx</span><span class="o">=</span><span class="n">traj_idx</span><span class="p">,</span>
            <span class="n">storage</span><span class="o">=</span><span class="n">storage</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_index</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">start_idx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">stop_idx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">seq_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">storage_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">traj_idx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">storage</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="c1"># end_point is the last possible index for start</span>
        <span class="n">last_indexable_start</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">seq_length</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">end_point</span> <span class="o">=</span> <span class="n">last_indexable_start</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">end_point</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">span_left</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">span_left</span> <span class="o">&gt;=</span> <span class="n">seq_length</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The right and left span must be strictly lower than the sequence length&quot;</span>
                <span class="p">)</span>
            <span class="n">end_point</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">span_left</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">start_point</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">start_point</span> <span class="o">=</span> <span class="o">-</span><span class="n">seq_length</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">span_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">span_right</span> <span class="o">&gt;=</span> <span class="n">seq_length</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The right and left span must be strictly lower than the sequence length&quot;</span>
                <span class="p">)</span>
            <span class="n">start_point</span> <span class="o">=</span> <span class="o">-</span><span class="n">span_right</span>

        <span class="n">relative_starts</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">lengths</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="p">)</span>
            <span class="o">*</span> <span class="p">(</span><span class="n">end_point</span> <span class="o">-</span> <span class="n">start_point</span><span class="p">)</span>
        <span class="p">)</span><span class="o">.</span><span class="n">floor</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">start_idx</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">+</span> <span class="n">start_point</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">out_of_traj</span> <span class="o">=</span> <span class="n">relative_starts</span> <span class="o">&lt;</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">out_of_traj</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="c1"># a negative start means sampling fewer elements</span>
                <span class="n">seq_length</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                    <span class="o">~</span><span class="n">out_of_traj</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">+</span> <span class="n">relative_starts</span>
                <span class="p">)</span>
                <span class="n">relative_starts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="o">~</span><span class="n">out_of_traj</span><span class="p">,</span> <span class="n">relative_starts</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">out_of_traj</span> <span class="o">=</span> <span class="n">relative_starts</span> <span class="o">+</span> <span class="n">seq_length</span> <span class="o">&gt;</span> <span class="n">lengths</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">out_of_traj</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="c1"># a negative start means sampling fewer elements</span>
                <span class="n">seq_length</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span>
                    <span class="n">seq_length</span><span class="p">,</span> <span class="n">lengths</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">relative_starts</span>
                <span class="p">)</span>

        <span class="n">starts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="p">(</span><span class="n">start_idx</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">relative_starts</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
                <span class="n">start_idx</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">:],</span>
            <span class="p">],</span>
            <span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_slices_from_startend</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">starts</span><span class="p">,</span> <span class="n">storage_length</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">truncated_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span>
            <span class="n">done_key</span> <span class="o">=</span> <span class="n">_replace_last</span><span class="p">(</span><span class="n">truncated_key</span><span class="p">,</span> <span class="s2">&quot;done&quot;</span><span class="p">)</span>
            <span class="n">terminated_key</span> <span class="o">=</span> <span class="n">_replace_last</span><span class="p">(</span><span class="n">truncated_key</span><span class="p">,</span> <span class="s2">&quot;terminated&quot;</span><span class="p">)</span>

            <span class="n">truncated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">index</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">truncated</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">truncated</span><span class="p">[</span><span class="n">seq_length</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">st_index</span> <span class="o">=</span> <span class="n">storage</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">done</span> <span class="o">=</span> <span class="n">st_index</span><span class="p">[</span><span class="n">done_key</span><span class="p">]</span> <span class="o">|</span> <span class="n">truncated</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="n">done</span> <span class="o">=</span> <span class="n">truncated</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">terminated</span> <span class="o">=</span> <span class="n">st_index</span><span class="p">[</span><span class="n">terminated_key</span><span class="p">]</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="n">terminated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">truncated</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">index</span><span class="p">,</span> <span class="p">{</span>
                <span class="n">truncated_key</span><span class="p">:</span> <span class="n">truncated</span><span class="p">,</span>
                <span class="n">done_key</span><span class="p">:</span> <span class="n">done</span><span class="p">,</span>
                <span class="n">terminated_key</span><span class="p">:</span> <span class="n">terminated</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">index</span><span class="p">,</span> <span class="p">{}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_used_traj_key</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;__used_traj_key&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">traj_key</span><span class="p">)</span>

    <span class="nd">@_used_traj_key</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">_used_traj_key</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;__used_traj_key&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_used_end_key</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;__used_end_key&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_key</span><span class="p">)</span>

    <span class="nd">@_used_end_key</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">_used_end_key</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;__used_end_key&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="c1"># no op - cache does not need to be saved</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="c1"># no op</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="o">...</span></div>


<div class="viewcode-block" id="SliceSamplerWithoutReplacement"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.SliceSamplerWithoutReplacement.html#torchrl.data.replay_buffers.SliceSamplerWithoutReplacement">[docs]</a><span class="k">class</span> <span class="nc">SliceSamplerWithoutReplacement</span><span class="p">(</span><span class="n">SliceSampler</span><span class="p">,</span> <span class="n">SamplerWithoutReplacement</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Samples slices of data along the first dimension, given start and stop signals, without replacement.</span>

<span class="sd">    In this context, ``without replacement`` means that the same element (NOT trajectory) will not be sampled twice</span>
<span class="sd">    before the counter is automatically reset. Within a single sample, however, only one slice of a given trajectory</span>
<span class="sd">    will appear (see example below).</span>

<span class="sd">    This class is to be used with static replay buffers or in between two</span>
<span class="sd">    replay buffer extensions. Extending the replay buffer will reset the</span>
<span class="sd">    the sampler, and continuous sampling without replacement is currently not</span>
<span class="sd">    allowed.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        drop_last (bool, optional): if ``True``, the last incomplete sample (if any) will be dropped.</span>
<span class="sd">            If ``False``, this last sample will be kept.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        num_slices (int): the number of slices to be sampled. The batch-size</span>
<span class="sd">            must be greater or equal to the ``num_slices`` argument. Exclusive</span>
<span class="sd">            with ``slice_len``.</span>
<span class="sd">        slice_len (int): the length of the slices to be sampled. The batch-size</span>
<span class="sd">            must be greater or equal to the ``slice_len`` argument and divisible</span>
<span class="sd">            by it. Exclusive with ``num_slices``.</span>
<span class="sd">        end_key (NestedKey, optional): the key indicating the end of a</span>
<span class="sd">            trajectory (or episode). Defaults to ``(&quot;next&quot;, &quot;done&quot;)``.</span>
<span class="sd">        traj_key (NestedKey, optional): the key indicating the trajectories.</span>
<span class="sd">            Defaults to ``&quot;episode&quot;`` (commonly used across datasets in TorchRL).</span>
<span class="sd">        ends (torch.Tensor, optional): a 1d boolean tensor containing the end of run signals.</span>
<span class="sd">            To be used whenever the ``end_key`` or ``traj_key`` is expensive to get,</span>
<span class="sd">            or when this signal is readily available. Must be used with ``cache_values=True``</span>
<span class="sd">            and cannot be used in conjunction with ``end_key`` or ``traj_key``.</span>
<span class="sd">        trajectories (torch.Tensor, optional): a 1d integer tensor containing the run ids.</span>
<span class="sd">            To be used whenever the ``end_key`` or ``traj_key`` is expensive to get,</span>
<span class="sd">            or when this signal is readily available. Must be used with ``cache_values=True``</span>
<span class="sd">            and cannot be used in conjunction with ``end_key`` or ``traj_key``.</span>
<span class="sd">        truncated_key (NestedKey, optional): If not ``None``, this argument</span>
<span class="sd">            indicates where a truncated signal should be written in the output</span>
<span class="sd">            data. This is used to indicate to value estimators where the provided</span>
<span class="sd">            trajectory breaks. Defaults to ``(&quot;next&quot;, &quot;truncated&quot;)``.</span>
<span class="sd">            This feature only works with :class:`~torchrl.data.replay_buffers.TensorDictReplayBuffer`</span>
<span class="sd">            instances (otherwise the truncated key is returned in the info dictionary</span>
<span class="sd">            returned by the :meth:`~torchrl.data.replay_buffers.ReplayBuffer.sample` method).</span>
<span class="sd">        strict_length (bool, optional): if ``False``, trajectories of length</span>
<span class="sd">            shorter than `slice_len` (or `batch_size // num_slices`) will be</span>
<span class="sd">            allowed to appear in the batch. If ``True``, trajectories shorted</span>
<span class="sd">            than required will be filtered out.</span>
<span class="sd">            Be mindful that this can result in effective `batch_size`  shorter</span>
<span class="sd">            than the one asked for! Trajectories can be split using</span>
<span class="sd">            :func:`~torchrl.collectors.split_trajectories`. Defaults to ``True``.</span>
<span class="sd">        shuffle (bool, optional): if ``False``, the order of the trajectories</span>
<span class="sd">            is not shuffled. Defaults to ``True``.</span>
<span class="sd">        compile (bool or dict of kwargs, optional): if ``True``, the bottleneck of</span>
<span class="sd">            the :meth:`~sample` method will be compiled with :func:`~torch.compile`.</span>
<span class="sd">            Keyword arguments can also be passed to torch.compile with this arg.</span>
<span class="sd">            Defaults to ``False``.</span>

<span class="sd">    .. note:: To recover the trajectory splits in the storage,</span>
<span class="sd">        :class:`~torchrl.data.replay_buffers.samplers.SliceSamplerWithoutReplacement` will first</span>
<span class="sd">        attempt to find the ``traj_key`` entry in the storage. If it cannot be</span>
<span class="sd">        found, the ``end_key`` will be used to reconstruct the episodes.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.replay_buffers import LazyMemmapStorage, TensorDictReplayBuffer</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.replay_buffers.samplers import SliceSamplerWithoutReplacement</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; rb = TensorDictReplayBuffer(</span>
<span class="sd">        ...     storage=LazyMemmapStorage(1000),</span>
<span class="sd">        ...     # asking for 10 slices for a total of 320 elements, ie, 10 trajectories of 32 transitions each</span>
<span class="sd">        ...     sampler=SliceSamplerWithoutReplacement(num_slices=10),</span>
<span class="sd">        ...     batch_size=320,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; episode = torch.zeros(1000, dtype=torch.int)</span>
<span class="sd">        &gt;&gt;&gt; episode[:300] = 1</span>
<span class="sd">        &gt;&gt;&gt; episode[300:550] = 2</span>
<span class="sd">        &gt;&gt;&gt; episode[550:700] = 3</span>
<span class="sd">        &gt;&gt;&gt; episode[700:] = 4</span>
<span class="sd">        &gt;&gt;&gt; data = TensorDict(</span>
<span class="sd">        ...     {</span>
<span class="sd">        ...         &quot;episode&quot;: episode,</span>
<span class="sd">        ...         &quot;obs&quot;: torch.randn((3, 4, 5)).expand(1000, 3, 4, 5),</span>
<span class="sd">        ...         &quot;act&quot;: torch.randn((20,)).expand(1000, 20),</span>
<span class="sd">        ...         &quot;other&quot;: torch.randn((20, 50)).expand(1000, 20, 50),</span>
<span class="sd">        ...     }, [1000]</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; rb.extend(data)</span>
<span class="sd">        &gt;&gt;&gt; sample = rb.sample()</span>
<span class="sd">        &gt;&gt;&gt; # since we want trajectories of 32 transitions but there are only 4 episodes to</span>
<span class="sd">        &gt;&gt;&gt; # sample from, we only get 4 x 32 = 128 transitions in this batch</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;sample:&quot;, sample)</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;trajectories in sample&quot;, sample.get(&quot;episode&quot;).unique())</span>

<span class="sd">    :class:`~torchrl.data.replay_buffers.SliceSamplerWithoutReplacement` is default-compatible with</span>
<span class="sd">    most of TorchRL&#39;s datasets, and allows users to consume datasets in a dataloader-like fashion:</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.datasets import RobosetExperienceReplay</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data import SliceSamplerWithoutReplacement</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; torch.manual_seed(0)</span>
<span class="sd">        &gt;&gt;&gt; num_slices = 10</span>
<span class="sd">        &gt;&gt;&gt; dataid = list(RobosetExperienceReplay.available_datasets)[0]</span>
<span class="sd">        &gt;&gt;&gt; data = RobosetExperienceReplay(dataid, batch_size=320,</span>
<span class="sd">        ...     sampler=SliceSamplerWithoutReplacement(num_slices=num_slices))</span>
<span class="sd">        &gt;&gt;&gt; # the last sample is kept, since drop_last=False by default</span>
<span class="sd">        &gt;&gt;&gt; for i, batch in enumerate(data):</span>
<span class="sd">        ...     print(batch.get(&quot;episode&quot;).unique())</span>
<span class="sd">        tensor([ 5,  6,  8, 11, 12, 14, 16, 17, 19, 24])</span>
<span class="sd">        tensor([ 1,  2,  7,  9, 10, 13, 15, 18, 21, 22])</span>
<span class="sd">        tensor([ 0,  3,  4, 20, 23])</span>

<span class="sd">    When requesting a large total number of samples with few trajectories and small span, the batch will contain</span>
<span class="sd">    only at most one sample of each trajectory:</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.collectors.utils import split_trajectories</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data import ReplayBuffer, LazyTensorStorage, SliceSampler, SliceSamplerWithoutReplacement</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; rb = ReplayBuffer(storage=LazyTensorStorage(max_size=1000),</span>
<span class="sd">        ...                   sampler=SliceSamplerWithoutReplacement(</span>
<span class="sd">        ...                       slice_len=5, traj_key=&quot;episode&quot;,strict_length=False</span>
<span class="sd">        ...                   ))</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; ep_1 = TensorDict(</span>
<span class="sd">        ...     {&quot;obs&quot;: torch.arange(100),</span>
<span class="sd">        ...     &quot;episode&quot;: torch.zeros(100),},</span>
<span class="sd">        ...     batch_size=[100]</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; ep_2 = TensorDict(</span>
<span class="sd">        ...     {&quot;obs&quot;: torch.arange(51),</span>
<span class="sd">        ...     &quot;episode&quot;: torch.ones(51),},</span>
<span class="sd">        ...     batch_size=[51]</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; rb.extend(ep_1)</span>
<span class="sd">        &gt;&gt;&gt; rb.extend(ep_2)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; s = rb.sample(50)</span>
<span class="sd">        &gt;&gt;&gt; t = split_trajectories(s, trajectory_key=&quot;episode&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(t[&quot;obs&quot;])</span>
<span class="sd">        tensor([[14, 15, 16, 17, 18],</span>
<span class="sd">                [ 3,  4,  5,  6,  7]])</span>
<span class="sd">        &gt;&gt;&gt; print(t[&quot;episode&quot;])</span>
<span class="sd">        tensor([[0., 0., 0., 0., 0.],</span>
<span class="sd">                [1., 1., 1., 1., 1.]])</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; s = rb.sample(50)</span>
<span class="sd">        &gt;&gt;&gt; t = split_trajectories(s, trajectory_key=&quot;episode&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(t[&quot;obs&quot;])</span>
<span class="sd">        tensor([[ 4,  5,  6,  7,  8],</span>
<span class="sd">                [26, 27, 28, 29, 30]])</span>
<span class="sd">        &gt;&gt;&gt; print(t[&quot;episode&quot;])</span>
<span class="sd">        tensor([[0., 0., 0., 0., 0.],</span>
<span class="sd">                [1., 1., 1., 1., 1.]])</span>


<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">slice_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">end_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">traj_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ends</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trajectories</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">truncated_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;truncated&quot;</span><span class="p">),</span>
        <span class="n">strict_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="nb">compile</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">SliceSampler</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">num_slices</span><span class="o">=</span><span class="n">num_slices</span><span class="p">,</span>
            <span class="n">slice_len</span><span class="o">=</span><span class="n">slice_len</span><span class="p">,</span>
            <span class="n">end_key</span><span class="o">=</span><span class="n">end_key</span><span class="p">,</span>
            <span class="n">traj_key</span><span class="o">=</span><span class="n">traj_key</span><span class="p">,</span>
            <span class="n">cache_values</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">truncated_key</span><span class="o">=</span><span class="n">truncated_key</span><span class="p">,</span>
            <span class="n">strict_length</span><span class="o">=</span><span class="n">strict_length</span><span class="p">,</span>
            <span class="n">ends</span><span class="o">=</span><span class="n">ends</span><span class="p">,</span>
            <span class="n">trajectories</span><span class="o">=</span><span class="n">trajectories</span><span class="p">,</span>
            <span class="nb">compile</span><span class="o">=</span><span class="nb">compile</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">SamplerWithoutReplacement</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="n">drop_last</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">perc</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span> <span class="o">*</span> <span class="mi">100</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">perc</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;num_slices=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;slice_len=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;end_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">end_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;traj_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">traj_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;truncated_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;strict_length=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">strict_length</span><span class="si">}</span><span class="s2">,&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">perc</span><span class="si">}</span><span class="s2">% sampled)&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">SamplerWithoutReplacement</span><span class="o">.</span><span class="n">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_storage_len</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage_len_buffer</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size_multiplier</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size_multiplier</span>
        <span class="n">start_idx</span><span class="p">,</span> <span class="n">stop_idx</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_stop_and_length</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span>
        <span class="c1"># we have to make sure that the number of dims of the storage</span>
        <span class="c1"># is the same as the stop/start signals since we will</span>
        <span class="c1"># use these to sample the storage</span>
        <span class="k">if</span> <span class="n">start_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">storage</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected the end-of-trajectory signal to be &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">storage</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">-dimensional. Got a </span><span class="si">{</span><span class="n">start_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> tensor &quot;</span>
                <span class="s2">&quot;instead.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_storage_len_buffer</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">start_idx</span><span class="p">)</span>
        <span class="c1"># first get indices of the trajectories we want to retrieve</span>
        <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_slices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_adjusted_batch_size</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">indices</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">SamplerWithoutReplacement</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">,</span> <span class="n">num_slices</span><span class="p">)</span>
        <span class="n">storage_length</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># traj_idx will either be a single tensor or a tuple that can be reorganized</span>
        <span class="c1"># like a non-zero through stacking.</span>
        <span class="k">def</span> <span class="nf">tuple_to_tensor</span><span class="p">(</span><span class="n">traj_idx</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">traj_idx</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">traj_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">lengths</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <span class="n">storage</span><span class="o">.</span><span class="n">shape</span>
                <span class="p">)[</span><span class="n">traj_idx</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">traj_idx</span>

        <span class="n">idx</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_slices</span><span class="p">(</span>
            <span class="n">lengths</span><span class="p">,</span>
            <span class="n">start_idx</span><span class="p">,</span>
            <span class="n">stop_idx</span><span class="p">,</span>
            <span class="n">seq_length</span><span class="p">,</span>
            <span class="n">num_slices</span><span class="p">,</span>
            <span class="n">storage_length</span><span class="p">,</span>
            <span class="n">traj_idx</span><span class="o">=</span><span class="n">tuple_to_tensor</span><span class="p">(</span><span class="n">indices</span><span class="p">),</span>
            <span class="n">storage</span><span class="o">=</span><span class="n">storage</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">idx</span><span class="p">,</span> <span class="n">info</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">SamplerWithoutReplacement</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">SamplerWithoutReplacement</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">)</span></div>


<div class="viewcode-block" id="PrioritizedSliceSampler"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.PrioritizedSliceSampler.html#torchrl.data.replay_buffers.PrioritizedSliceSampler">[docs]</a><span class="k">class</span> <span class="nc">PrioritizedSliceSampler</span><span class="p">(</span><span class="n">SliceSampler</span><span class="p">,</span> <span class="n">PrioritizedSampler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Samples slices of data along the first dimension, given start and stop signals, using prioritized sampling.</span>

<span class="sd">    This class samples sub-trajectories with replacement following a priority weighting presented in &quot;Schaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2015.</span>
<span class="sd">        Prioritized experience replay.&quot;</span>
<span class="sd">        (https://arxiv.org/abs/1511.05952)</span>

<span class="sd">    For more info see :class:`~torchrl.data.replay_buffers.samplers.SliceSampler` and :class:`~torchrl.data.replay_buffers.samplers.PrioritizedSampler`.</span>

<span class="sd">    .. warning:: PrioritizedSliceSampler will look at the priorities of the individual transitions and sample the</span>
<span class="sd">        start points accordingly. This means that transitions with a low priority may as well appear in the</span>
<span class="sd">        samples if they follow another of higher priority, and transitions with a high priority but closer to the</span>
<span class="sd">        end of a trajectory may never be sampled if they cannot be used as start points.</span>
<span class="sd">        Currently, it is the user responsibility to aggregate priorities across items of a trajectory using</span>
<span class="sd">        :meth:`~.update_priority`.</span>

<span class="sd">    Args:</span>
<span class="sd">        alpha (:obj:`float`): exponent α determines how much prioritization is used,</span>
<span class="sd">            with α = 0 corresponding to the uniform case.</span>
<span class="sd">        beta (:obj:`float`): importance sampling negative exponent.</span>
<span class="sd">        eps (:obj:`float`, optional): delta added to the priorities to ensure that the buffer</span>
<span class="sd">            does not contain null priorities. Defaults to 1e-8.</span>
<span class="sd">        reduction (str, optional): the reduction method for multidimensional</span>
<span class="sd">            tensordicts (i.e., stored trajectory). Can be one of &quot;max&quot;, &quot;min&quot;,</span>
<span class="sd">            &quot;median&quot; or &quot;mean&quot;.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        num_slices (int): the number of slices to be sampled. The batch-size</span>
<span class="sd">            must be greater or equal to the ``num_slices`` argument. Exclusive</span>
<span class="sd">            with ``slice_len``.</span>
<span class="sd">        slice_len (int): the length of the slices to be sampled. The batch-size</span>
<span class="sd">            must be greater or equal to the ``slice_len`` argument and divisible</span>
<span class="sd">            by it. Exclusive with ``num_slices``.</span>
<span class="sd">        end_key (NestedKey, optional): the key indicating the end of a</span>
<span class="sd">            trajectory (or episode). Defaults to ``(&quot;next&quot;, &quot;done&quot;)``.</span>
<span class="sd">        traj_key (NestedKey, optional): the key indicating the trajectories.</span>
<span class="sd">            Defaults to ``&quot;episode&quot;`` (commonly used across datasets in TorchRL).</span>
<span class="sd">        ends (torch.Tensor, optional): a 1d boolean tensor containing the end of run signals.</span>
<span class="sd">            To be used whenever the ``end_key`` or ``traj_key`` is expensive to get,</span>
<span class="sd">            or when this signal is readily available. Must be used with ``cache_values=True``</span>
<span class="sd">            and cannot be used in conjunction with ``end_key`` or ``traj_key``.</span>
<span class="sd">        trajectories (torch.Tensor, optional): a 1d integer tensor containing the run ids.</span>
<span class="sd">            To be used whenever the ``end_key`` or ``traj_key`` is expensive to get,</span>
<span class="sd">            or when this signal is readily available. Must be used with ``cache_values=True``</span>
<span class="sd">            and cannot be used in conjunction with ``end_key`` or ``traj_key``.</span>
<span class="sd">        cache_values (bool, optional): to be used with static datasets.</span>
<span class="sd">            Will cache the start and end signal of the trajectory. This can be safely used even</span>
<span class="sd">            if the trajectory indices change during calls to :class:`~torchrl.data.ReplayBuffer.extend`</span>
<span class="sd">            as this operation will erase the cache.</span>

<span class="sd">            .. warning:: ``cache_values=True`` will not work if the sampler is used with a</span>
<span class="sd">                storage that is extended by another buffer. For instance:</span>

<span class="sd">                    &gt;&gt;&gt; buffer0 = ReplayBuffer(storage=storage,</span>
<span class="sd">                    ...     sampler=SliceSampler(num_slices=8, cache_values=True),</span>
<span class="sd">                    ...     writer=ImmutableWriter())</span>
<span class="sd">                    &gt;&gt;&gt; buffer1 = ReplayBuffer(storage=storage,</span>
<span class="sd">                    ...     sampler=other_sampler)</span>
<span class="sd">                    &gt;&gt;&gt; # Wrong! Does not erase the buffer from the sampler of buffer0</span>
<span class="sd">                    &gt;&gt;&gt; buffer1.extend(data)</span>

<span class="sd">            .. warning:: ``cache_values=True`` will not work as expected if the buffer is</span>
<span class="sd">                shared between processes and one process is responsible for writing</span>
<span class="sd">                and one process for sampling, as erasing the cache can only be done locally.</span>

<span class="sd">        truncated_key (NestedKey, optional): If not ``None``, this argument</span>
<span class="sd">            indicates where a truncated signal should be written in the output</span>
<span class="sd">            data. This is used to indicate to value estimators where the provided</span>
<span class="sd">            trajectory breaks. Defaults to ``(&quot;next&quot;, &quot;truncated&quot;)``.</span>
<span class="sd">            This feature only works with :class:`~torchrl.data.replay_buffers.TensorDictReplayBuffer`</span>
<span class="sd">            instances (otherwise the truncated key is returned in the info dictionary</span>
<span class="sd">            returned by the :meth:`~torchrl.data.replay_buffers.ReplayBuffer.sample` method).</span>
<span class="sd">        strict_length (bool, optional): if ``False``, trajectories of length</span>
<span class="sd">            shorter than `slice_len` (or `batch_size // num_slices`) will be</span>
<span class="sd">            allowed to appear in the batch. If ``True``, trajectories shorted</span>
<span class="sd">            than required will be filtered out.</span>
<span class="sd">            Be mindful that this can result in effective `batch_size`  shorter</span>
<span class="sd">            than the one asked for! Trajectories can be split using</span>
<span class="sd">            :func:`~torchrl.collectors.split_trajectories`. Defaults to ``True``.</span>
<span class="sd">        compile (bool or dict of kwargs, optional): if ``True``, the bottleneck of</span>
<span class="sd">            the :meth:`~sample` method will be compiled with :func:`~torch.compile`.</span>
<span class="sd">            Keyword arguments can also be passed to torch.compile with this arg.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        span (bool, int, Tuple[bool | int, bool | int], optional): if provided, the sampled</span>
<span class="sd">            trajectory will span across the left and/or the right. This means that possibly</span>
<span class="sd">            fewer elements will be provided than what was required. A boolean value means</span>
<span class="sd">            that at least one element will be sampled per trajectory. An integer `i` means</span>
<span class="sd">            that at least `slice_len - i` samples will be gathered for each sampled trajectory.</span>
<span class="sd">            Using tuples allows a fine grained control over the span on the left (beginning</span>
<span class="sd">            of the stored trajectory) and on the right (end of the stored trajectory).</span>
<span class="sd">        max_priority_within_buffer (bool, optional): if ``True``, the max-priority</span>
<span class="sd">            is tracked within the buffer. When ``False``, the max-priority tracks</span>
<span class="sd">            the maximum value since the instantiation of the sampler.</span>
<span class="sd">            Defaults to ``False``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.replay_buffers import TensorDictReplayBuffer, LazyMemmapStorage, PrioritizedSliceSampler</span>
<span class="sd">        &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">        &gt;&gt;&gt; sampler = PrioritizedSliceSampler(max_capacity=9, num_slices=3, alpha=0.7, beta=0.9)</span>
<span class="sd">        &gt;&gt;&gt; rb = TensorDictReplayBuffer(storage=LazyMemmapStorage(9), sampler=sampler, batch_size=6)</span>
<span class="sd">        &gt;&gt;&gt; data = TensorDict(</span>
<span class="sd">        ...     {</span>
<span class="sd">        ...         &quot;observation&quot;: torch.randn(9,16),</span>
<span class="sd">        ...         &quot;action&quot;: torch.randn(9, 1),</span>
<span class="sd">        ...         &quot;episode&quot;: torch.tensor([0,0,0,1,1,1,2,2,2], dtype=torch.long),</span>
<span class="sd">        ...         &quot;steps&quot;: torch.tensor([0,1,2,0,1,2,0,1,2], dtype=torch.long),</span>
<span class="sd">        ...         (&quot;next&quot;, &quot;observation&quot;): torch.randn(9,16),</span>
<span class="sd">        ...         (&quot;next&quot;, &quot;reward&quot;): torch.randn(9,1),</span>
<span class="sd">        ...         (&quot;next&quot;, &quot;done&quot;): torch.tensor([0,0,1,0,0,1,0,0,1], dtype=torch.bool).unsqueeze(1),</span>
<span class="sd">        ...     },</span>
<span class="sd">        ...     batch_size=[9],</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; rb.extend(data)</span>
<span class="sd">        &gt;&gt;&gt; sample, info = rb.sample(return_info=True)</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;episode&quot;, sample[&quot;episode&quot;].tolist())</span>
<span class="sd">        episode [2, 2, 2, 2, 1, 1]</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;steps&quot;, sample[&quot;steps&quot;].tolist())</span>
<span class="sd">        steps [1, 2, 0, 1, 1, 2]</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;weight&quot;, info[&quot;_weight&quot;].tolist())</span>
<span class="sd">        weight [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</span>
<span class="sd">        &gt;&gt;&gt; priority = torch.tensor([0,3,3,0,0,0,1,1,1])</span>
<span class="sd">        &gt;&gt;&gt; rb.update_priority(torch.arange(0,9,1), priority=priority)</span>
<span class="sd">        &gt;&gt;&gt; sample, info = rb.sample(return_info=True)</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;episode&quot;, sample[&quot;episode&quot;].tolist())</span>
<span class="sd">        episode [2, 2, 2, 2, 2, 2]</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;steps&quot;, sample[&quot;steps&quot;].tolist())</span>
<span class="sd">        steps [1, 2, 0, 1, 0, 1]</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;weight&quot;, info[&quot;_weight&quot;].tolist())</span>
<span class="sd">        weight [9.120110917137936e-06, 9.120110917137936e-06, 9.120110917137936e-06, 9.120110917137936e-06, 9.120110917137936e-06, 9.120110917137936e-06]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">max_capacity</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;max&quot;</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">slice_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">end_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">traj_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ends</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trajectories</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_values</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncated_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;truncated&quot;</span><span class="p">),</span>
        <span class="n">strict_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="nb">compile</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">span</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">bool</span> <span class="o">|</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">max_priority_within_buffer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">SliceSampler</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">num_slices</span><span class="o">=</span><span class="n">num_slices</span><span class="p">,</span>
            <span class="n">slice_len</span><span class="o">=</span><span class="n">slice_len</span><span class="p">,</span>
            <span class="n">end_key</span><span class="o">=</span><span class="n">end_key</span><span class="p">,</span>
            <span class="n">traj_key</span><span class="o">=</span><span class="n">traj_key</span><span class="p">,</span>
            <span class="n">cache_values</span><span class="o">=</span><span class="n">cache_values</span><span class="p">,</span>
            <span class="n">truncated_key</span><span class="o">=</span><span class="n">truncated_key</span><span class="p">,</span>
            <span class="n">strict_length</span><span class="o">=</span><span class="n">strict_length</span><span class="p">,</span>
            <span class="n">ends</span><span class="o">=</span><span class="n">ends</span><span class="p">,</span>
            <span class="n">trajectories</span><span class="o">=</span><span class="n">trajectories</span><span class="p">,</span>
            <span class="nb">compile</span><span class="o">=</span><span class="nb">compile</span><span class="p">,</span>
            <span class="n">span</span><span class="o">=</span><span class="n">span</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">PrioritizedSampler</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">max_capacity</span><span class="o">=</span><span class="n">max_capacity</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span>
            <span class="n">max_priority_within_buffer</span><span class="o">=</span><span class="n">max_priority_within_buffer</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="c1"># Span left is hard to achieve because we need to sample &#39;negative&#39; starts, but to sample</span>
            <span class="c1"># the start we rely on PrioritizedSampler which has no idea it&#39;s looking at trajectories.</span>
            <span class="c1">#</span>
            <span class="c1"># Another way to go about this would be to stochastically decrease the seq_length to</span>
            <span class="c1"># accommodate this but that would require to over-sample the starts too.</span>
            <span class="c1">#</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Left spanning is disabled for </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> and will be automatically turned off. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;If this feature is required, please file an issue on torchrl GitHub repo.&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">span</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;num_slices=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;slice_len=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;end_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">end_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;traj_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">traj_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;truncated_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;strict_length=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">strict_length</span><span class="si">}</span><span class="s2">,&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;alpha=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;beta=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_beta</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;eps=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_eps</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">SliceSampler</span><span class="o">.</span><span class="n">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">state</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">PrioritizedSampler</span><span class="o">.</span><span class="n">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">mark_update</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="o">*</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">PrioritizedSampler</span><span class="o">.</span><span class="n">mark_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">storage</span><span class="o">=</span><span class="n">storage</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_padded_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shapes</span><span class="p">,</span> <span class="n">arange</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># this complex mumbo jumbo creates a left padded tensor with valid indices on the right, e.g.</span>
        <span class="c1"># tensor([[ 0,  1,  2,  3,  4],</span>
        <span class="c1">#         [-1, -1,  5,  6,  7],</span>
        <span class="c1">#         [-1,  8,  9, 10, 11]])</span>
        <span class="c1"># where the -1 items on the left are padded values</span>
        <span class="n">num_groups</span> <span class="o">=</span> <span class="n">shapes</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">max_group_len</span> <span class="o">=</span> <span class="n">shapes</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">pad_lengths</span> <span class="o">=</span> <span class="n">max_group_len</span> <span class="o">-</span> <span class="n">shapes</span>

        <span class="c1"># Get all the start and end indices within arange for each group</span>
        <span class="n">group_ends</span> <span class="o">=</span> <span class="n">shapes</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">group_starts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">group_ends</span><span class="p">)</span>
        <span class="n">group_starts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">group_starts</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">group_ends</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="p">(</span><span class="n">num_groups</span><span class="p">,</span> <span class="n">max_group_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">arange</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">arange</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">pad_row</span><span class="p">,</span> <span class="n">group_start</span><span class="p">,</span> <span class="n">group_end</span><span class="p">,</span> <span class="n">pad_len</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="n">pad</span><span class="p">,</span> <span class="n">group_starts</span><span class="p">,</span> <span class="n">group_ends</span><span class="p">,</span> <span class="n">pad_lengths</span>
        <span class="p">):</span>
            <span class="n">pad_row</span><span class="p">[:</span><span class="n">pad_len</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">pad_row</span><span class="p">[</span><span class="n">pad_len</span><span class="p">:]</span> <span class="o">=</span> <span class="n">arange</span><span class="p">[</span><span class="n">group_start</span><span class="p">:</span><span class="n">group_end</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">pad</span>

    <span class="k">def</span> <span class="nf">_preceding_stop_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">start_idx</span><span class="p">):</span>
        <span class="n">preceding_stop_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;preceding_stop_idx&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">preceding_stop_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">preceding_stop_idx</span>
        <span class="n">arange</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">storage</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>
        <span class="n">shapes</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">shapes</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">==</span> <span class="n">arange</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Wrong shapes / arange configuration&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">strict_length</span><span class="p">:</span>
            <span class="c1"># First, remove the starts from the arange</span>
            <span class="c1"># We do this because each traj can be sampled</span>
            <span class="n">all_but_starts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">arange</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
            <span class="n">starts</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">starts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">starts</span><span class="p">[:</span><span class="mi">1</span><span class="p">]),</span> <span class="n">starts</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
            <span class="n">all_but_starts</span><span class="p">[</span><span class="n">starts</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">arange</span> <span class="o">=</span> <span class="n">arange</span><span class="p">[</span><span class="n">all_but_starts</span><span class="p">]</span>
            <span class="n">shapes</span> <span class="o">=</span> <span class="n">shapes</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padded_indices</span><span class="p">(</span><span class="n">shapes</span><span class="p">,</span> <span class="n">arange</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">span_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">span_right</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">span_right</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="n">preceding_stop_idx</span> <span class="o">=</span> <span class="n">pad</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Mask the rightmost values of that padded tensor</span>
            <span class="n">preceding_stop_idx</span> <span class="o">=</span> <span class="n">pad</span><span class="p">[:,</span> <span class="o">-</span><span class="n">seq_length</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">span_right</span> <span class="p">:]</span>
        <span class="n">preceding_stop_idx</span> <span class="o">=</span> <span class="n">preceding_stop_idx</span><span class="p">[</span><span class="n">preceding_stop_idx</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">storage</span><span class="o">.</span><span class="n">_is_full</span><span class="p">:</span>
            <span class="n">preceding_stop_idx</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">preceding_stop_idx</span>
                <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel_multi_index</span><span class="p">(</span>
                    <span class="nb">tuple</span><span class="p">(</span><span class="n">start_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()),</span> <span class="n">storage</span><span class="o">.</span><span class="n">_total_shape</span>
                <span class="p">)</span>
            <span class="p">)</span> <span class="o">%</span> <span class="n">storage</span><span class="o">.</span><span class="n">_total_shape</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_values</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="s2">&quot;preceding_stop_idx&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">preceding_stop_idx</span>
        <span class="k">return</span> <span class="n">preceding_stop_idx</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="c1"># Sample `batch_size` indices representing the start of a slice.</span>
        <span class="c1"># The sampling is based on a weight vector.</span>
        <span class="n">start_idx</span><span class="p">,</span> <span class="n">stop_idx</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_stop_and_length</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span>
        <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_slices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_adjusted_batch_size</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="n">preceding_stop_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_preceding_stop_idx</span><span class="p">(</span>
            <span class="n">storage</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">start_idx</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">storage</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># we need to convert indices of the permuted, flatten storage to indices in a flatten storage (not permuted)</span>
            <span class="c1"># This is because the lengths come as they would for a permuted storage</span>
            <span class="n">preceding_stop_idx</span> <span class="o">=</span> <span class="n">unravel_index</span><span class="p">(</span>
                <span class="n">preceding_stop_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">storage</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">storage</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="p">)</span>
            <span class="n">preceding_stop_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">preceding_stop_idx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">preceding_stop_idx</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">preceding_stop_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">ravel_multi_index</span><span class="p">(</span><span class="n">preceding_stop_idx</span><span class="p">,</span> <span class="n">storage</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># force to not sample index at the end of a trajectory</span>
        <span class="n">vals</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">[</span><span class="n">preceding_stop_idx</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">[</span><span class="n">preceding_stop_idx</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="c1"># and no need to update self._min_tree</span>

        <span class="n">starts</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">PrioritizedSampler</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="o">=</span><span class="n">storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">seq_length</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">[</span><span class="n">preceding_stop_idx</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span> <span class="o">=</span> <span class="n">vals</span>
        <span class="c1"># We must truncate the seq_length if (1) not strict length or (2) span[1]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">strict_length</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">starts</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">starts_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">starts</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">stop_idx</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">starts_tensor</span> <span class="o">=</span> <span class="n">starts</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">stop_idx</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="c1"># Find the stop that comes after the start index</span>
            <span class="c1"># say start_tensor has shape [N, X] and stop_idx has shape [M, X]</span>
            <span class="c1"># diff will have shape [M, N, X]</span>
            <span class="n">stop_idx_corr</span> <span class="o">=</span> <span class="n">stop_idx</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="n">stop_idx_corr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                <span class="n">stop_idx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">start_idx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
                <span class="n">stop_idx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">storage</span><span class="o">.</span><span class="n">_len_along_dim0</span><span class="p">,</span>
                <span class="n">stop_idx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="n">stop_idx_corr</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">starts_tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># filter out all items that don&#39;t belong to the same dim in the storage</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">diff</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="n">diff</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="n">diff</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">starts_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="c1"># We remove all neg values from consideration</span>
            <span class="n">diff</span><span class="p">[</span><span class="n">diff</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="c1"># Take the arg min along dim 0 (thereby reducing dim M)</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">stops</span> <span class="o">=</span> <span class="n">stop_idx_corr</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="c1"># TODO: here things may not work bc we could have spanning trajs,</span>
            <span class="c1">#  though I cannot show that it breaks in the tests</span>
            <span class="k">if</span> <span class="n">starts_tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">starts_tensor</span> <span class="o">=</span> <span class="n">starts_tensor</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="n">seq_length</span> <span class="o">=</span> <span class="p">(</span><span class="n">stops</span> <span class="o">-</span> <span class="n">starts_tensor</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">clamp_max</span><span class="p">(</span><span class="n">seq_length</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">seq_length</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;failed to compute seq_length, please report this bug&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">starts</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">starts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">starts</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># starts = torch.as_tensor(starts, device=lengths.device)</span>
        <span class="n">info</span><span class="p">[</span><span class="s2">&quot;_weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">info</span><span class="p">[</span><span class="s2">&quot;_weight&quot;</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">lengths</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># extends starting indices of each slice with sequence_length to get indices of all steps</span>
        <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_slices_from_startend</span><span class="p">(</span>
            <span class="n">seq_length</span><span class="p">,</span> <span class="n">starts</span><span class="p">,</span> <span class="n">storage_length</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># repeat the weight of each slice to match the number of steps</span>
        <span class="n">info</span><span class="p">[</span><span class="s2">&quot;_weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">info</span><span class="p">[</span><span class="s2">&quot;_weight&quot;</span><span class="p">],</span> <span class="n">seq_length</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># following logics borrowed from SliceSampler</span>
            <span class="n">truncated_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span>

            <span class="n">done_key</span> <span class="o">=</span> <span class="n">_replace_last</span><span class="p">(</span><span class="n">truncated_key</span><span class="p">,</span> <span class="s2">&quot;done&quot;</span><span class="p">)</span>
            <span class="n">terminated_key</span> <span class="o">=</span> <span class="n">_replace_last</span><span class="p">(</span><span class="n">truncated_key</span><span class="p">,</span> <span class="s2">&quot;terminated&quot;</span><span class="p">)</span>

            <span class="n">truncated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">index</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">truncated</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">truncated</span><span class="p">[</span><span class="n">seq_length</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">st_index</span> <span class="o">=</span> <span class="n">storage</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">done</span> <span class="o">=</span> <span class="n">st_index</span><span class="p">[</span><span class="n">done_key</span><span class="p">]</span> <span class="o">|</span> <span class="n">truncated</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="n">done</span> <span class="o">=</span> <span class="n">truncated</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">terminated</span> <span class="o">=</span> <span class="n">st_index</span><span class="p">[</span><span class="n">terminated_key</span><span class="p">]</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="n">terminated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">truncated</span><span class="p">)</span>
            <span class="n">info</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="n">truncated_key</span><span class="p">:</span> <span class="n">truncated</span><span class="p">,</span>
                    <span class="n">done_key</span><span class="p">:</span> <span class="n">done</span><span class="p">,</span>
                    <span class="n">terminated_key</span><span class="p">:</span> <span class="n">terminated</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">index</span><span class="p">,</span> <span class="n">info</span>
        <span class="k">return</span> <span class="n">index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">info</span>

    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># no op for SliceSampler</span>
        <span class="n">PrioritizedSampler</span><span class="o">.</span><span class="n">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="c1"># no op for SliceSampler</span>
        <span class="n">PrioritizedSampler</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="c1"># no op for SliceSampler</span>
        <span class="k">return</span> <span class="n">PrioritizedSampler</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># no op for SliceSampler</span>
        <span class="k">return</span> <span class="n">PrioritizedSampler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">PrioritizedSampler</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">SliceSampler</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">PrioritizedSampler</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">SliceSampler</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span></div>


<div class="viewcode-block" id="SamplerEnsemble"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.SamplerEnsemble.html#torchrl.data.replay_buffers.SamplerEnsemble">[docs]</a><span class="k">class</span> <span class="nc">SamplerEnsemble</span><span class="p">(</span><span class="n">Sampler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;An ensemble of samplers.</span>

<span class="sd">    This class is designed to work with :class:`~torchrl.data.replay_buffers.replay_buffers.ReplayBufferEnsemble`.</span>
<span class="sd">    It contains the samplers as well as the sampling strategy hyperparameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        samplers (sequence of Sampler): the samplers to make the composite sampler.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        p (list or tensor of probabilities, optional): if provided, indicates the</span>
<span class="sd">            weights of each dataset during sampling.</span>
<span class="sd">        sample_from_all (bool, optional): if ``True``, each dataset will be sampled</span>
<span class="sd">            from. This is not compatible with the ``p`` argument. Defaults to ``False``.</span>
<span class="sd">        num_buffer_sampled (int, optional): the number of buffers to sample.</span>
<span class="sd">            if ``sample_from_all=True``, this has no effect, as it defaults to the</span>
<span class="sd">            number of buffers. If ``sample_from_all=False``, buffers will be</span>
<span class="sd">            sampled according to the probabilities ``p``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">      The indices provided in the info dictionary are placed in a :class:`~tensordict.TensorDict` with</span>
<span class="sd">      keys ``index`` and ``buffer_ids`` that allow the upper :class:`~torchrl.data.ReplayBufferEnsemble`</span>
<span class="sd">      and :class:`~torchrl.data.StorageEnsemble` objects to retrieve the data.</span>
<span class="sd">      This format is different from with other samplers which usually return indices</span>
<span class="sd">      as regular tensors.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">samplers</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_from_all</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_buffer_sampled</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_rng_private</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span> <span class="o">=</span> <span class="n">samplers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_all</span> <span class="o">=</span> <span class="n">sample_from_all</span>
        <span class="k">if</span> <span class="n">sample_from_all</span> <span class="ow">and</span> <span class="n">p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot pass both `p` argument and `sample_from_all=True`.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_buffer_sampled</span> <span class="o">=</span> <span class="n">num_buffer_sampled</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_rng</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rng_private</span>

    <span class="nd">@_rng</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">_rng</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_rng_private</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">for</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">:</span>
            <span class="n">sampler</span><span class="o">.</span><span class="n">_rng</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">p</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_p</span>

    <span class="nd">@p</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">p</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">value</span> <span class="o">/</span> <span class="n">value</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_p</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">num_buffer_sampled</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_num_buffer_sampled&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;_num_buffer_sampled&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value</span>

    <span class="nd">@num_buffer_sampled</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">num_buffer_sampled</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;_num_buffer_sampled&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buffer_sampled</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">StorageEnsemble</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span>
        <span class="n">sub_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buffer_sampled</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_all</span><span class="p">:</span>
            <span class="n">samples</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span>
                <span class="o">*</span><span class="p">[</span>
                    <span class="n">sampler</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">sub_batch_size</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">storage</span><span class="p">,</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">storage</span><span class="o">.</span><span class="n">_storages</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">)</span>
                <span class="p">]</span>
            <span class="p">)</span>
            <span class="n">buffer_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">buffer_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">),</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buffer_sampled</span><span class="p">,),</span>
                    <span class="n">generator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="s2">&quot;device&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">buffer_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buffer_sampled</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="n">samples</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span>
                <span class="o">*</span><span class="p">[</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">storage</span><span class="o">.</span><span class="n">_storages</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sub_batch_size</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">buffer_ids</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
                <span class="p">]</span>
            <span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">sample</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">samples</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">sample</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="n">samples_stack</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">samples_stack</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;index&quot;</span><span class="p">:</span> <span class="n">samples_stack</span><span class="p">,</span>
                <span class="s2">&quot;buffer_ids&quot;</span><span class="p">:</span> <span class="n">buffer_ids</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buffer_sampled</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">infos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">TensorDict</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="n">samples</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">info</span>
                <span class="k">else</span> <span class="n">TensorDict</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">infos</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">samples</span><span class="p">,</span> <span class="n">infos</span>

    <span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="n">Path</span><span class="p">):</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">absolute</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">):</span>
            <span class="n">sampler</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="n">Path</span><span class="p">):</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">absolute</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">):</span>
            <span class="n">sampler</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">):</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">):</span>
            <span class="n">sampler</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="n">_INDEX_ERROR</span> <span class="o">=</span> <span class="s2">&quot;Expected an index of type torch.Tensor, range, np.ndarray, int, slice or ellipsis, got </span><span class="si">{}</span><span class="s2"> instead.&quot;</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">Ellipsis</span><span class="p">:</span>
                <span class="n">index</span> <span class="o">=</span> <span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">index</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Tuple of length greater than 1 are not accepted to index samplers of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">result</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">slice</span><span class="p">)</span> <span class="ow">and</span> <span class="n">index</span> <span class="o">==</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">range</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">index</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Cannot index a </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2"> with tensor indices that have more than one dimension.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">index</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;A floating point index was recieved when an integer dtype was expected.&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">slice</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">index</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_INDEX_ERROR</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">index</span><span class="p">)))</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_INDEX_ERROR</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">index</span><span class="p">)))</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
            <span class="n">samplers</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">index</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># slice</span>
            <span class="n">samplers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_p</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">SamplerEnsemble</span><span class="p">(</span>
            <span class="o">*</span><span class="n">samplers</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
            <span class="n">sample_from_all</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_from_all</span><span class="p">,</span>
            <span class="n">num_buffer_sampled</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buffer_sampled</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">samplers</span> <span class="o">=</span> <span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;samplers=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="se">\n</span><span class="si">{</span><span class="n">samplers</span><span class="si">}</span><span class="s2">)&quot;</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>


        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
     
    <script type="text/javascript">
      $(document).ready(function() {
	  var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
	  if (downloadNote.length >= 1) {
	      var tutorialUrl = $("#tutorial-type").text();
	      var githubLink = "https://github.com/pytorch/rl/blob/main/tutorials/sphinx-tutorials/"  + tutorialUrl + ".py",
		  notebookLink = $(".sphx-glr-download-jupyter").find(".download.reference")[0].href,
		  notebookDownloadPath = notebookLink.split('_downloads')[1],
		  colabLink = "https://colab.research.google.com/github/pytorch/rl/blob/gh-pages/main/_downloads" + notebookDownloadPath;

	      $(".pytorch-call-to-action-links a[data-response='Run in Google Colab']").attr("href", colabLink);
	      $(".pytorch-call-to-action-links a[data-response='View on Github']").attr("href", githubLink);
	  }

          var overwrite = function(_) {
              if ($(this).length > 0) {
                  $(this)[0].href = "https://github.com/pytorch/rl"
              }
          }
          // PC
          $(".main-menu a:contains('GitHub')").each(overwrite);
          // Mobile
          $(".main-menu a:contains('Github')").each(overwrite);
      });

      
       $(window).ready(function() {
           var original = window.sideMenus.bind;
           var startup = true;
           window.sideMenus.bind = function() {
               original();
               if (startup) {
                   $("#pytorch-right-menu a.reference.internal").each(function(i) {
                       if (this.classList.contains("not-expanded")) {
                           this.nextElementSibling.style.display = "block";
                           this.classList.remove("not-expanded");
                           this.classList.add("expanded");
                       }
                   });
                   startup = false;
               }
           };
       });
    </script>

    


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>